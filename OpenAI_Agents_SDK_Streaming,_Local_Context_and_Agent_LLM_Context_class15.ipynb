{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOajO5aJM3Qy7pWoJhcXWES",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZainAli24/OpenAI_Agents_SDK_class_02/blob/main/OpenAI_Agents_SDK_Streaming%2C_Local_Context_and_Agent_LLM_Context_class15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ijklz_DOfP7O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI-Agents SDK mien **Streaming** do tarah se hoti hai: **1. Agent-Loop Level pe Streaming , 2. LLM Request ke Waqt Streaming:**\n",
        "\n",
        "OpenAI-Agent SDK mien do tarah ki streaming hoti hai ik Agent-Loop level pe streaming hoti hai aur dosra jab LLM ko request bheji jati hai tab bhi streaming hoti hai. Let me isko thoda detail mein explain karun:\n",
        "\n",
        "### 1. **Agent-Loop Level pe Streaming**\n",
        "Agent-loop level pe streaming ka matlab hai ke jab agent apna poora process chala raha hota hai—jaise tools ko call karna, doosre agents ko handoff karna, ya apne actions ko execute karna—to aapko real-time updates milte hain. Yeh updates `stream_events()` method ke zariye events ki shakal mein aate hain, jaise:\n",
        "\n",
        "- **`run_item_stream_event`**: Isme agent ke different steps ke bare mein information hoti hai, maslan:\n",
        "  - `tool_call_item`: Jab koi tool call kiya jata hai.\n",
        "  - `tool_call_output_item`: Jab tool ka output milta hai.\n",
        "  - `message_output_item`: Jab koi message output generate hota hai.\n",
        "- **`agent_updated_stream_event`**: Yeh tab trigger hota hai jab agent update hota hai ya kaam doosre agent ko transfer karta hai.\n",
        "\n",
        "Iska faida yeh hai ke aap agent ke har qadam ko real-time mein monitor kar sakte hain aur user ko progress dikhaya ja sakta hai.\n",
        "\n",
        "### 2. **LLM Request ke Waqt Streaming**\n",
        "Jab agent LLM (Large Language Model) ko request bhejta hai, to LLM ka response bhi streaming ke zariye incrementally milta hai. Yeh `raw_response_event` ke through hota hai, jisme LLM ka output token-by-token ya chhote chunks mein receive hota hai. Yeh feature real-time applications ke liye bohot useful hai, kyunki aapko poora response complete hone ka wait nahi karna padta—jaise hi response generate hota hai, waise hi aapko milna shuru ho jata hai.\n",
        "\n",
        "### Yeh Kaise Kaam Karta Hai?\n",
        "OpenAI Agents SDK mein streaming ek unified mechanism ke zariye kaam karti hai. Jab aap `Runner.run_streamed()` call karte hain, to yeh ek `RunResultStreaming` object return karta hai. Iske baad, `result.stream_events()` ko use karke aap events ka stream access kar sakte hain. Har event ka type alag hota hai (jaise `raw_response_event`, `run_item_stream_event`, ya `agent_updated_stream_event`), aur aap inko apne zarurat ke mutabiq handle kar sakte hain.\n",
        "\n",
        "### Example Code\n",
        "Yeh ek chhota sa code snippet hai jo yeh dikhata hai:\n",
        "\n",
        "```python\n",
        "async for event in result.stream_events():\n",
        "    if event.type == \"raw_response_event\":\n",
        "        print(event.data.delta, end=\"\", flush=True)  # LLM response ka ek hissa\n",
        "    elif event.type == \"run_item_stream_event\":\n",
        "        if event.item.type == \"tool_call_item\":\n",
        "            print(\"-- Tool was called\")\n",
        "        elif event.item.type == \"tool_call_output_item\":\n",
        "            print(f\"-- Tool output: {event.item.output}\")\n",
        "```\n",
        "\n",
        "Is example mein:\n",
        "- `raw_response_event` se LLM ka response real-time mein print ho raha hai.\n",
        "- `run_item_stream_event` se tool calls aur unke outputs ke updates mil rahe hain.\n",
        "\n",
        "### Conclusion\n",
        "To aapka kehna bilkul durust hai—OpenAI Agents SDK mein streaming dono tarah se hoti hai:\n",
        "- **Agent-loop level pe**, jahan agent ke execution ke har qadam ke updates milte hain.\n",
        "- **LLM request ke waqt**, jahan LLM ka response incrementally stream hota hai.\n",
        "\n",
        "Yeh dono ek hi streaming system ka hissa hain jo different types ke events provide karta hai, lekin aap inko alag-alag tarah se samajh aur use kar sakte hain apne application ke liye."
      ],
      "metadata": {
        "id": "KHERMQz7k7FO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------\n",
        "\n",
        "## **LLM Request Streaming Confusion:**\n",
        "#### Ab baat karte hain LLM (Large Language Model) request ki. Jab agent ko koi final output dena hota hai—jaise user ke sawaal ka jawab—tab woh LLM ko ek request bhejta hai aur phir LLM us ka reponse generate karta hai. Jab LLM response generate karta hai, to woh ek baar mein poora jawab nahi bhejta. Balki, yeh **token-by-token** ya chhote-chhote chunks mein aata hai. Iska matlab hai ke jaise-jaise LLM response banata hai, aapko woh real-time mein milta hai—har word ya token ek-ek karke. Is process ko hum **LLM request pe streaming** kehte hain, aur yeh `raw_response_event` ke through dikhta hai.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-jE75fiDq-Q4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gTDnu9LymMcM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Streaming:**"
      ],
      "metadata": {
        "id": "4cYcimED6N0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq openai-agents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-fKzz2A3N7r",
        "outputId": "3cdd8fff-3823-48ff-929a-0ea792a25358"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.5/119.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make your Jupyter Notebook capable of running asynchronous functions."
      ],
      "metadata": {
        "id": "wNyCAKiW3X0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "TpnADN3j3S5y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Google Gemini with OPENAI-Agent SDK"
      ],
      "metadata": {
        "id": "ZQFvL8vr3cUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Check ke **result** mien kia kia cheezay a rahi hai"
      ],
      "metadata": {
        "id": "2uzvVumu4bGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from agents import AsyncOpenAI, Agent, Runner, OpenAIChatCompletionsModel, RunConfig\n",
        "from google.colab import userdata\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "import asyncio\n",
        "\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "modeler = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client\n",
        ")\n",
        "\n",
        "async def main():\n",
        "  agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You are a helpful assistant.\",\n",
        "      model=modeler,\n",
        "  )\n",
        "\n",
        "  result = Runner.run_streamed(\n",
        "      agent,\n",
        "      \"Please make 5 python beginner level quiz for me!\"\n",
        "  )\n",
        "  print(type(result), result)  # result output\n",
        "\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANTI7oPP3k8Y",
        "outputId": "439b9bf0-a15b-419b-8d77-05b719003a97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'agents.result.RunResultStreaming'> RunResultStreaming:\n",
            "- Current agent: Agent(name=\"Assistant\", ...)\n",
            "- Current turn: 0\n",
            "- Max turns: 10\n",
            "- Is complete: False\n",
            "- Final output (NoneType):\n",
            "    None\n",
            "- 0 new item(s)\n",
            "- 0 raw response(s)\n",
            "- 0 input guardrail result(s)\n",
            "- 0 output guardrail result(s)\n",
            "(See `RunResultStreaming` for more details)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Simple Final Text Streaming:**\n",
        "\n"
      ],
      "metadata": {
        "id": "Sw32M0Ug3FyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from agents import AsyncOpenAI, Agent, Runner, OpenAIChatCompletionsModel, RunConfig, set_tracing_disabled\n",
        "from google.colab import userdata\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "import asyncio\n",
        "\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "set_tracing_disabled(True)\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "modeler = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client\n",
        ")\n",
        "\n",
        "async def main():\n",
        "  agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You are a helpful assistant.\",\n",
        "      model=modeler,\n",
        "  )\n",
        "\n",
        "  result = Runner.run_streamed(\n",
        "      agent,\n",
        "      \"Please make 5 python beginner level quiz for me!\"\n",
        "  )\n",
        "  async for event in result.stream_events():\n",
        "    if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
        "      print(event.data.delta, end=\"\", flush=True)  # LLM ka final response\n",
        "\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpWKd7oT59sH",
        "outputId": "ce088c10-7afb-4127-9cac-39be40b97412"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here are 5 beginner-level Python quizzes. Each quiz focuses on a specific core concept and is designed to be easy to understand for someone new to programming.\n",
            "\n",
            "**Quiz 1: Variables and Data Types**\n",
            "\n",
            "```python\n",
            "# Quiz 1: Variables and Data Types\n",
            "\n",
            "print(\"Quiz 1: Variables and Data Types\")\n",
            "\n",
            "# Question 1:  What data type is best suited for storing a whole number?\n",
            "# a) str   b) float   c) int   d) bool\n",
            "answer1 = input"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:openai.agents:OPENAI_API_KEY is not set, skipping trace export\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"Question 1: Choose a, b, c, or d: \")\n",
            "if answer1 == \"c\":\n",
            "    print(\"Correct!\")\n",
            "    score = 1\n",
            "else:\n",
            "    print(\"Incorrect. The answer is c) int\")\n",
            "    score = 0\n",
            "\n",
            "\n",
            "# Question 2: How do you assign the value \"Hello\" to a variable named 'greeting'?\n",
            "# a) greeting = Hello   b) greeting == \"Hello\"   c) greeting := \"Hello\"   d) greeting = \"Hello\"\n",
            "answer2 = input(\"Question 2: Choose a, b, c, or d: \")\n",
            "\n",
            "if answer2 == \"d\":\n",
            "    print(\"Correct!\")\n",
            "    score += 1\n",
            "else:\n",
            "    print(\"Incorrect. The answer is d) greeting = 'Hello'\")\n",
            "\n",
            "# Question 3: What is the data type of `True`?\n",
            "# a) string b) integer c) boolean d) float\n",
            "\n",
            "answer3 = input(\"Question 3: Choose a, b, c, or d: \")\n",
            "if answer3 == 'c':\n",
            "  print(\"Correct!\")\n",
            "  score += 1\n",
            "else:\n",
            "  print(\"Incorrect. The answer is c) boolean\")\n",
            "\n",
            "# Output the score.\n",
            "print(f'Your score for Quiz 1 is {score} / 3')\n",
            "```\n",
            "\n",
            "**Quiz 2: Basic Input and Output**\n",
            "\n",
            "```python\n",
            "# Quiz 2: Basic Input and Output\n",
            "\n",
            "print(\"\\nQuiz 2: Basic Input and Output\")\n",
            "\n",
            "# Question 1: How do you display the message \"Welcome!\" on the screen in Python?\n",
            "# a) print \"Welcome!\"   b) display(\"Welcome!\")   c) echo(\"Welcome!\")  d) print(\"Welcome!\")\n",
            "answer1 = input(\"Question 1: Choose a, b, c, or d: \")\n",
            "\n",
            "if answer1 == \"d\":\n",
            "    print(\"Correct!\")\n",
            "    score = 1\n",
            "else:\n",
            "    print(\"Incorrect. The answer is d) print('Welcome!')\")\n",
            "    score = 0\n",
            "\n",
            "# Question 2: Which function is used to get input from the user?\n",
            "# a) output()   b) get()   c) input()   d) read()\n",
            "answer2 = input(\"Question 2: Choose a, b, c, or d: \")\n",
            "\n",
            "if answer2 == \"c\":\n",
            "    print(\"Correct!\")\n",
            "    score += 1\n",
            "else:\n",
            "    print(\"Incorrect. The answer is c) input()\")\n",
            "\n",
            "# Question 3: What will the following code print?\n",
            "# name = input(\"Enter your name: \")\n",
            "# print(\"Hello, \" + name)\n",
            "# a) Error b) Hello,  c) Hello, name d) Depends on the user input.\n",
            "\n",
            "answer3 = input(\"Question 3: Choose a, b, c, or d: \")\n",
            "\n",
            "if answer3 == \"d\":\n",
            "  print(\"Correct!\")\n",
            "  score += 1\n",
            "else:\n",
            "  print(\"Incorrect. The answer is d) Depends on the user input.\")\n",
            "\n",
            "print(f'Your score for Quiz 2 is {score} / 3')\n",
            "```\n",
            "\n",
            "**Quiz 3: Operators**\n",
            "\n",
            "```python\n",
            "# Quiz 3: Operators\n",
            "\n",
            "print(\"\\nQuiz 3: Operators\")\n",
            "\n",
            "# Question 1: What is the result of 5 + 3 * 2?\n",
            "# a) 16   b) 11   c) 8   d) 10\n",
            "answer1 = input(\"Question 1: Choose a, b, c, or d: \")\n",
            "\n",
            "if answer1 == \"b\":\n",
            "    print(\"Correct!\")\n",
            "    score = 1\n",
            "else:\n",
            "    print(\"Incorrect. The answer is b) 11\")\n",
            "    score = 0\n",
            "\n",
            "# Question 2: Which operator is used for integer division (division that discards the remainder)?\n",
            "# a) /   b) %   c) //   d) **\n",
            "answer2 = input(\"Question 2: Choose a, b, c, or d: \")\n",
            "\n",
            "if answer2 == \"c\":\n",
            "    print(\"Correct!\")\n",
            "    score += 1\n",
            "else:\n",
            "    print(\"Incorrect. The answer is c) //\")\n",
            "\n",
            "# Question 3: What is the result of 10 % 3?\n",
            "# a) 3.333  b) 1  c) 0  d) 3\n",
            "answer3 = input(\"Question 3: Choose a, b, c, or d: \")\n",
            "\n",
            "if answer3 == \"b\":\n",
            "  print(\"Correct!\")\n",
            "  score += 1\n",
            "else:\n",
            "  print(\"Incorrect. The answer is b) 1\")\n",
            "\n",
            "print(f'Your score for Quiz 3 is {score} / 3')\n",
            "```\n",
            "\n",
            "**Quiz 4: Conditional Statements (if/else)**\n",
            "\n",
            "```python\n",
            "# Quiz 4: Conditional Statements (if/else)\n",
            "\n",
            "print(\"\\nQuiz 4: Conditional Statements (if/else)\")\n",
            "\n",
            "# Question 1:  What keyword is used to start a conditional block in Python?\n",
            "# a) when   b) if   c) for   d) while\n",
            "answer1 = input(\"Question 1: Choose a, b, c, or d: \")\n",
            "\n",
            "if answer1 == \"b\":\n",
            "    print(\"Correct!\")\n",
            "    score = 1\n",
            "else:\n",
            "    print(\"Incorrect. The answer is b) if\")\n",
            "    score = 0\n",
            "\n",
            "# Question 2: What will the following code print if x = 5?\n",
            "# x = 5\n",
            "# if x > 10:\n",
            "#     print(\"x is greater than 10\")\n",
            "# else:\n",
            "#     print(\"x is not greater than 10\")\n",
            "\n",
            "# a) x is greater than 10  b) x is not greater than 10  c) Error  d) 5\n",
            "answer2 = input(\"Question 2: Choose a, b, c, or d: \")\n",
            "\n",
            "if answer2 == \"b\":\n",
            "    print(\"Correct!\")\n",
            "    score += 1\n",
            "else:\n",
            "    print(\"Incorrect. The answer is b) x is not greater than 10\")\n",
            "\n",
            "# Question 3: What is the purpose of the `else` statement in an if-else block?\n",
            "# a) To define a new condition.\n",
            "# b) To execute code only if the `if` condition is true.\n",
            "# c) To execute code only if the `if` condition is false.\n",
            "# d) To end the if-else block.\n",
            "\n",
            "answer3 = input(\"Question 3: Choose a, b, c, or d: \")\n",
            "if answer3 == 'c':\n",
            "  print(\"Correct!\")\n",
            "  score += 1\n",
            "else:\n",
            "  print(\"Incorrect. The answer is c) To execute code only if the `if` condition is false.\")\n",
            "\n",
            "print(f'Your score for Quiz 4 is {score} / 3')\n",
            "```\n",
            "\n",
            "**Quiz 5:  Loops (for loop)**\n",
            "\n",
            "```python\n",
            "# Quiz 5: Loops (for loop)\n",
            "\n",
            "print(\"\\nQuiz 5: Loops (for loop)\")\n",
            "\n",
            "# Question 1: What type of loop is best suited to iterate over a sequence of numbers?\n",
            "# a) while   b) if   c) for   d) else\n",
            "answer1 = input(\"Question 1: Choose a, b, c, or d: \")\n",
            "\n",
            "if answer1 == \"c\":\n",
            "    print(\"Correct!\")\n",
            "    score = 1\n",
            "else:\n",
            "    print(\"Incorrect. The answer is c) for\")\n",
            "    score = 0\n",
            "\n",
            "# Question 2: How many times will the following loop execute?\n",
            "# for i in range(5):\n",
            "#     print(i)\n",
            "# a) 4   b) 6   c) 5   d) 0\n",
            "answer2 = input(\"Question 2: Choose a, b, c, or d: \")\n",
            "\n",
            "if answer2 == \"c\":\n",
            "    print(\"Correct!\")\n",
            "    score += 1\n",
            "else:\n",
            "    print(\"Incorrect. The answer is c) 5\")\n",
            "\n",
            "# Question 3: What will be the last value printed by the following loop?\n",
            "# for i in range(2, 6):\n",
            "#     print(i)\n",
            "\n",
            "# a) 2 b) 5 c) 6 d) Error\n",
            "answer3 = input(\"Question 3: Choose a, b, c, or d: \")\n",
            "\n",
            "if answer3 == 'b':\n",
            "  print(\"Correct!\")\n",
            "  score += 1\n",
            "else:\n",
            "  print(\"Incorrect. The answer is b) 5\")\n",
            "\n",
            "print(f'Your score for Quiz 5 is {score} / 3')\n",
            "```\n",
            "\n",
            "**How to Use These Quizzes**\n",
            "\n",
            "1.  **Copy and Paste:** Copy each quiz into a separate Python file (e.g., `quiz1.py`, `quiz2.py`, etc.).\n",
            "2.  **Run the File:** Execute each Python file from your terminal using `python quiz1.py` (replace `quiz1.py` with the appropriate filename).\n",
            "3.  **Answer the Questions:** The program will ask you questions and you'll enter your answer (a, b, c, or d).\n",
            "4.  **See Your Score:** After each quiz, your score will be displayed.\n",
            "\n",
            "These quizzes are designed to give you a basic assessment of your understanding of fundamental Python concepts.  Good luck! Let me know if you'd like more quizzes or explanations on any of the topics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Agent-Loop level Streaming:**\n"
      ],
      "metadata": {
        "id": "WXjeIqXn7Gvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import AsyncOpenAI, Agent, Runner, OpenAIChatCompletionsModel, RunConfig, function_tool, ItemHelpers\n",
        "from google.colab import userdata\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "import asyncio\n",
        "\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "modeler = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client\n",
        ")\n",
        "\n",
        "@function_tool\n",
        "def age_Guesser(dob: int):\n",
        "  return f\"Your Age is: {2025 - dob}\"\n",
        "\n",
        "async def main():\n",
        "  agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You are a Expert in user age gusse.\",\n",
        "      model=modeler,\n",
        "      tools=[age_Guesser]\n",
        "  )\n",
        "\n",
        "  result = Runner.run_streamed(\n",
        "      agent,\n",
        "      \"I was born in 2004, Please tell me what is my age?\"\n",
        "  )\n",
        "  async for event in result.stream_events():\n",
        "    print(type(event))\n",
        "\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPHhh_tN7z4s",
        "outputId": "1057221f-21de-46c7-becc-7258be6991f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'agents.stream_events.AgentUpdatedStreamEvent'>\n",
            "<class 'agents.stream_events.RawResponsesStreamEvent'>\n",
            "<class 'agents.stream_events.RawResponsesStreamEvent'>\n",
            "<class 'agents.stream_events.RawResponsesStreamEvent'>\n",
            "<class 'agents.stream_events.RawResponsesStreamEvent'>\n",
            "<class 'agents.stream_events.RawResponsesStreamEvent'>\n",
            "<class 'agents.stream_events.RunItemStreamEvent'>\n",
            "<class 'agents.stream_events.RunItemStreamEvent'>\n",
            "<class 'agents.stream_events.RawResponsesStreamEvent'>\n",
            "<class 'agents.stream_events.RawResponsesStreamEvent'>\n",
            "<class 'agents.stream_events.RawResponsesStreamEvent'>\n",
            "<class 'agents.stream_events.RawResponsesStreamEvent'>\n",
            "<class 'agents.stream_events.RawResponsesStreamEvent'>\n",
            "<class 'agents.stream_events.RawResponsesStreamEvent'>\n",
            "<class 'agents.stream_events.RawResponsesStreamEvent'>\n",
            "<class 'agents.stream_events.RawResponsesStreamEvent'>\n",
            "<class 'agents.stream_events.RunItemStreamEvent'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "async def main():\n",
        "  agent = Agent(\n",
        "      name=\"AgeGuesser\",\n",
        "      instructions=\"You are a Expert in user age gusse. the current year is 2025, use age_Guesser tool for this\",\n",
        "      model=modeler,\n",
        "      tools=[age_Guesser]\n",
        "  )\n",
        "\n",
        "  result = Runner.run_streamed(\n",
        "      agent,\n",
        "      \"I was born in 2004, Please tell me what is my age?\"\n",
        "  )\n",
        "  async for event in result.stream_events():\n",
        "    print(event)\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41eNSbteRUlr",
        "outputId": "baf253e6-b977-42b2-d000-b859d9cc1986"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='AgeGuesser', instructions='You are a Expert in user age gusse. the current year is 2025, use age_Guesser tool for this', handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7ce92f1a5050>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None), tools=[FunctionTool(name='age_Guesser', description='', params_json_schema={'properties': {'dob': {'title': 'Dob', 'type': 'integer'}}, 'required': ['dob'], 'title': 'age_Guesser_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7ce92f1c4b80>, strict_json_schema=True)], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1747045732.6305585, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseFunctionToolCall(arguments='{\"dob\":2004}', call_id='', name='age_Guesser', type='function_call', id='__fake_id__', status=None), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='{\"dob\":2004}', item_id='__fake_id__', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseFunctionToolCall(arguments='{\"dob\":2004}', call_id='', name='age_Guesser', type='function_call', id='__fake_id__', status=None), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1747045732.6305585, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseFunctionToolCall(arguments='{\"dob\":2004}', call_id='', name='age_Guesser', type='function_call', id='__fake_id__', status=None)], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='tool_called', item=ToolCallItem(agent=Agent(name='AgeGuesser', instructions='You are a Expert in user age gusse. the current year is 2025, use age_Guesser tool for this', handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7ce92f1a5050>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None), tools=[FunctionTool(name='age_Guesser', description='', params_json_schema={'properties': {'dob': {'title': 'Dob', 'type': 'integer'}}, 'required': ['dob'], 'title': 'age_Guesser_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7ce92f1c4b80>, strict_json_schema=True)], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseFunctionToolCall(arguments='{\"dob\":2004}', call_id='', name='age_Guesser', type='function_call', id='__fake_id__', status=None), type='tool_call_item'), type='run_item_stream_event')\n",
            "RunItemStreamEvent(name='tool_output', item=ToolCallOutputItem(agent=Agent(name='AgeGuesser', instructions='You are a Expert in user age gusse. the current year is 2025, use age_Guesser tool for this', handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7ce92f1a5050>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None), tools=[FunctionTool(name='age_Guesser', description='', params_json_schema={'properties': {'dob': {'title': 'Dob', 'type': 'integer'}}, 'required': ['dob'], 'title': 'age_Guesser_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7ce92f1c4b80>, strict_json_schema=True)], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item={'call_id': '', 'output': 'Your Age is: 21', 'type': 'function_call_output'}, output='Your Age is: 21', type='tool_call_output_item'), type='run_item_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1747045732.9926343, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='Your', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' age is 21.\\n', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='Your age is 21.\\n', type='output_text'), type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='Your age is 21.\\n', type='output_text')], role='assistant', status='completed', type='message'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1747045732.9926343, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='Your age is 21.\\n', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='AgeGuesser', instructions='You are a Expert in user age gusse. the current year is 2025, use age_Guesser tool for this', handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7ce92f1a5050>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None), tools=[FunctionTool(name='age_Guesser', description='', params_json_schema={'properties': {'dob': {'title': 'Dob', 'type': 'integer'}}, 'required': ['dob'], 'title': 'age_Guesser_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7ce92f1c4b80>, strict_json_schema=True)], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='Your age is 21.\\n', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "  agent = Agent(\n",
        "      name=\"AgeGuesser\",\n",
        "      instructions=\"You are a Expert in user age gusse. the current year is 2025, First use tool to answer query and you will answer user very frankly\",\n",
        "      model=modeler,\n",
        "      tools=[age_Guesser]\n",
        "  )\n",
        "\n",
        "  result = Runner.run_streamed(\n",
        "      agent,\n",
        "      \"I was born in 1954 + 50 , Please tell me what is my age?\"\n",
        "  )\n",
        "  async for event in result.stream_events():\n",
        "    if event.type == \"raw_response_event\":\n",
        "      continue\n",
        "    elif event.type == \"agent_updated_stream_event\":\n",
        "      print(f\"Agent updated: {event.new_agent.name}\")\n",
        "      continue\n",
        "    elif event.type == \"run_item_stream_event\":\n",
        "      # print(event.item.type)\n",
        "      if event.item.type == \"tool_call_item\":\n",
        "        print(\"-- Tool was called --\")\n",
        "      elif event.item.type == \"tool_call_output_item\":\n",
        "        print(f\"-- Tool output: {event.item.output} --\")\n",
        "      elif event.item.type == \"message_output_item\":\n",
        "        print(f\"Message output: {ItemHelpers.text_message_output(event.item)}\")\n",
        "      else:\n",
        "        pass\n",
        "\n",
        "\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr4ZtMYrToTb",
        "outputId": "b34c00a1-2391-446f-92d2-aa72d97eec69"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent updated: AgeGuesser\n",
            "-- Tool was called --\n",
            "-- Tool output: Your Age is: 21 --\n",
            "Message output: Based on my calculations, you are 21 years old.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Confusion in Use of Continue in if-else statement:**\n",
        "\n",
        "Aapka sawal bilkul sahi hai: *\"Agar loop laga hua hai toh woh har event ki value ko check karega, toh phir `continue` kyun lagaya hai jab woh waise bhi har event ki type ko check kar raha tha?\"* Chalo isko simple aur step-by-step samajhte hain.\n",
        "\n",
        "---\n",
        "\n",
        "### **Loop Ka Kaam**\n",
        "Jab aap ek loop lagate hain jo har event ko process karta hai, toh haan, yeh sahi hai ke loop har event ki value ko check karega. Maslan, agar aapka code kuch aisa hai:\n",
        "\n",
        "```python\n",
        "for event in events:\n",
        "    if event.type == \"raw_response_event\":\n",
        "        continue\n",
        "    elif event.type == \"agent_updated_stream_event\":\n",
        "        print(f\"Agent updated: {event.new_agent.name}\")\n",
        "        continue\n",
        "    elif event.type == \"run_item_stream_event\":\n",
        "        # Nested conditions\n",
        "```\n",
        "\n",
        "Toh loop har event ko dekhega, aur har event ke liye conditions (`if`, `elif`) check karega. Lekin sawal yeh hai ke `continue` ki zarurat kyun padi jab loop waise bhi agli event pe jata hai?\n",
        "\n",
        "---\n",
        "\n",
        "### **Continue Ka Matlab**\n",
        "`continue` ek aisa statement hai jo loop ke current iteration ko rok deta hai aur seedha agli iteration pe le jata hai, bina baaki code ko chalaye. Yani, agar aap `continue` likhte hain, toh us condition ke baad loop ka jo bhi code bacha hai, woh skip ho jata hai aur agli event pe chale jate hain.\n",
        "\n",
        "Ab aapke case mein, `if-elif-else` structure ki wajah se baat thodi alag hai. Chalo dekhte hain.\n",
        "\n",
        "---\n",
        "\n",
        "### **Aapke Code Mein Continue Ka Role**\n",
        "Aapke code mein `continue` ka use kuch aisa hai:\n",
        "\n",
        "1. **`if event.type == \"raw_response_event\":`**\n",
        "   - Agar event ka type `\"raw_response_event\"` hai, to `continue` bolta hai: *\"Is event ke liye kuch mat karo, seedha agli event pe jao.\"*\n",
        "   - **Kya hota hai?**: Baaki conditions (`agent_updated_stream_event`, `run_item_stream_event`) check hi nahi hoti, aur loop agli event pe chala jata hai.\n",
        "\n",
        "2. **`elif event.type == \"agent_updated_stream_event\":`**\n",
        "   - Agar event ka type `\"agent_updated_stream_event\"` hai, to pehle `print` statement chalta hai, phir `continue` execute hota hai.\n",
        "   - **Kya hota hai?**: Print hone ke baad, baaki code skip ho jata hai, aur loop agli event pe jata hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Continue Kyun Lagaya Jab Loop Waise Bhi Check Kar Raha Hai?**\n",
        "Aapka point bilkul sahi hai ke loop har event ko check karta hai, aur `if-elif-else` structure ki wajah se ek baar mein sirf ek condition hi execute hoti hai. Yani:\n",
        "\n",
        "- Agar `event.type == \"raw_response_event\"` true hai, to `elif` wale parts (jaise `agent_updated_stream_event`) check nahi hote, kyunki `if-elif-else` mein ek condition true hone ke baad baaki skip ho jati hain.\n",
        "- Toh phir `continue` ki zarurat kya hai? Kyunki `if-elif-else` already yeh ensure karta hai ke baaki conditions check na hon.\n",
        "\n",
        "**Example Bina Continue Ke:**\n",
        "Agar aap `continue` hata den, to code aisa hoga:\n",
        "\n",
        "```python\n",
        "for event in events:\n",
        "    if event.type == \"raw_response_event\":\n",
        "        pass  # Kuch nahi karna\n",
        "    elif event.type == \"agent_updated_stream_event\":\n",
        "        print(f\"Agent updated: {event.new_agent.name}\")\n",
        "    elif event.type == \"run_item_stream_event\":\n",
        "        # Nested conditions\n",
        "```\n",
        "\n",
        "- Yeh code bhi bilkul sahi kaam karega. Kyunki `if-elif-else` structure ki wajah se, agar ek condition match ho jati hai (jaise `raw_response_event`), to baaki conditions check nahi hoti, aur loop khud hi agli event pe chala jata hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Toh Continue Kyun Use Kiya?**\n",
        "Agar `if-elif-else` structure mein `continue` ke bina bhi kaam ho sakta hai, to phir isko kyun lagaya gaya? Iske do bade reasons hain:\n",
        "\n",
        "1. **Code Ko Clear Banana:**\n",
        "   - `continue` likhne se yeh saaf ho jata hai ke aap current event ke liye processing yahin khatam karna chahte hain aur agli event pe jana chahte hain. Yeh ek tarah ka signal hai code padhne wale ke liye ke \"Yahan rukna nahi hai, agla event dekho.\"\n",
        "\n",
        "2. **Future Mein Safety:**\n",
        "   - Agar aap baad mein loop ke andar aur code add karte hain (jaise koi logging ya extra processing), to `continue` ensure karta hai ke woh code specific events (jaise `raw_response_event`) ke liye na chale. Bina `continue` ke, aapko manually har jagah `pass` ya empty block likhna pad sakta hai.\n",
        "\n",
        "**Misal:**\n",
        "Agar aap loop mein yeh add karte hain:\n",
        "\n",
        "```python\n",
        "for event in events:\n",
        "    if event.type == \"raw_response_event\":\n",
        "        # Bina continue ke\n",
        "        pass\n",
        "    elif event.type == \"agent_updated_stream_event\":\n",
        "        print(f\"Agent updated: {event.new_agent.name}\")\n",
        "    print(\"Processing complete\")  # New line\n",
        "```\n",
        "\n",
        "- Ab `raw_response_event` ke liye bhi \"Processing complete\" print hoga, jo shayad aap nahi chahte. Lekin agar `continue` hota, to yeh line skip ho jati.\n",
        "\n",
        "```python\n",
        "for event in events:\n",
        "    if event.type == \"raw_response_event\":\n",
        "        continue  # Yeh line baaki code ko skip kar degi\n",
        "    elif event.type == \"agent_updated_stream_event\":\n",
        "        print(f\"Agent updated: {event.new_agent.name}\")\n",
        "        continue\n",
        "    print(\"Processing complete\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "- **Aapka Point:** Sahi hai ke loop har event ko check karta hai, aur `if-elif-else` structure ki wajah se `continue` ke bina bhi code kaam karega.\n",
        "- **Continue Ka Fayda:** `continue` code ko zyada padhne mein aasan banata hai aur aapka intent clear karta hai ke aap ek event ko skip karna chahte hain. Plus, future mein agar code badhe, to mistakes se bachata hai.\n",
        "- **Zaruri Hai Ya Nahi?** Is case mein `continue` optional hai, kyunki `if-elif-else` already kaam kar raha hai. Lekin iska use karna ek achhi practice hai.\n",
        "\n",
        "Toh, aapke code mein `continue` lagana theek hai, aur yeh output pe koi galat asar nahi dalega. Agar aap ise hata den, to bhi code chalega, bas thodi clarity kam ho sakti hai. Aap tension na len—dono tarike sahi hain!"
      ],
      "metadata": {
        "id": "nVtSjMqq87DU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qwzpIZe29NEd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Context ka Concept:**\n",
        "\n",
        "Aapne apne query mein context ka concept explain kiya hai aur poocha hai ke kya aapne isko sahi samjha hai. Main aapke points ko padh kar yeh confirm karunga ke aapka samajhna bilkul sahi hai ya nahi. Chalo, step-by-step dekhte hain!\n",
        "\n",
        "---\n",
        "\n",
        "### **Aapka Samajhna**\n",
        "Aapne kaha hai ke:\n",
        "- **Context ka matlab**: Woh information jo LLM (Agent) ke paas nahi hoti, ya jo external information hoti hai, usko hum context kehte hain.\n",
        "- **Context dene ke teen tareeke**:\n",
        "  1. **System Prompt** ke through.\n",
        "  2. **User Prompt** ke through.\n",
        "  3. **External Functions** ke through (jaise web search, RAG system, tool calls, ya koi API).\n",
        "- **Context ka purpose**: Jab bhi koi information chahiye ho, to usko extract ya retrieve karke LLM ko provide kar diya jaye.\n",
        "\n",
        "Ab main aapke in points ko validate karunga aur bataunga ke aapne yeh concept sahi samjha hai ya nahi.\n",
        "\n",
        "---\n",
        "\n",
        "### **Context Kya Hai?**\n",
        "Context ka matlab hai woh additional information jo ek LLM ke paas by default nahi hoti, lekin jo usko kisi task ko perform karne ya sahi jawab dene ke liye chahiye hoti hai. Yeh information model ke training data se alag hoti hai aur dynamically (jab zarurat ho) provide ki jati hai. Aapne yeh point bilkul sahi samjha hai ke context external information hoti hai jo LLM ke knowledge ko supplement karti hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Aapke Teen Tareekon Ki Validation**\n",
        "Aapne context dene ke jo teen tareeke bataye hain, unko ek-ek karke dekhte hain:\n",
        "\n",
        "#### **1. System Prompt**\n",
        "- **Kya Hai?**  \n",
        "  System prompt ek tarah ka instruction set hota hai jo hum LLM ko dete hain taaki usko yeh pata chale ke uska role kya hai ya woh kaise behave kare. Isme hum external information bhi daal sakte hain jo LLM ke paas pehle se nahi hoti.\n",
        "- **Kaise Kaam Karta Hai?**  \n",
        "  Hum system prompt mein directly woh information likh dete hain jo LLM ko samajhna ya use karna chahiye. Yeh information poore conversation ke liye set ho jati hai aur har jawab ke liye available hoti hai.\n",
        "- **Example Ke Saath**  \n",
        "  Jaise agar hum likhein: \"Tum ek doctor ho aur tumhare paas 2023 ke latest medical research ka data hai: [insert data]. Iske base par jawab do.\" Yahan \"2023 ka medical research data\" ek external information hai jo LLM ke paas pehle nahi thi, aur system prompt ke through usko di gayi hai.\n",
        "- **Matlab**  \n",
        "  System prompt se hum LLM ko ek bada context dete hain jo uske poore kaam karne ke tareeke ko guide karta hai.\n",
        "\n",
        "#### **2. User Prompt**\n",
        "- **Kya Hai?**  \n",
        "  User prompt woh sawal ya input hota hai jo hum LLM ko directly dete hain. Isme bhi hum external information de sakte hain, jaise koi text ya file ke through, aur LLM us information ke base par humein jawab deta hai.\n",
        "- **Kaise Kaam Karta Hai?**  \n",
        "  Hum user prompt mein ya to information khud likh dete hain ya phir koi file attach karte hain (agar system allow karta hai). LLM phir us information ko padhta hai aur uske base par jawab banata hai. Kai systems mein file upload karne par uska content automatically prompt mein add ho jata hai.\n",
        "- **Example Ke Saath**  \n",
        "  Jaise agar hum kahein: \"Yeh meri company ka sales report hai: [insert report ya file]. Iske base par mujhe batao ke humara performance kaisa raha?\" Yahan \"sales report\" external information hai jo user prompt ke through di gayi, aur LLM usko use karke jawab deta hai.\n",
        "- **Matlab**  \n",
        "  User prompt se hum specific sawal ke liye specific information dete hain, jo sirf usi jawab ke liye kaam aati hai.\n",
        "\n",
        "### **Dono Mein Kya Farq Hai?**\n",
        "- **System Prompt**: Yeh LLM ko ek overall knowledge ya role deta hai jo har sawal ke jawab ke liye use hota hai. Isme external information static hoti hai aur poore conversation ke liye apply hoti hai.\n",
        "- **User Prompt**: Yeh sirf ek particular sawal ke liye information deta hai. Har sawal ke saath nayi information ya file de sakte hain, aur LLM usko bas usi jawab ke liye use karta hai.\n",
        "\n",
        "\n",
        "#### **3. External Functions**\n",
        "- **Kya hota hai?**: Yeh woh tareeke hain jinse LLM external sources se information retrieve karta hai. Ismein shamil ho sakta hai:\n",
        "  - **Web Search**: Internet se latest data lena.\n",
        "  - **RAG System**: Documents ya databases se relevant information nikalna.\n",
        "  - **Tool Calls/APIs**: Kisi specific tool ya API (jaise weather API) se data fetch karna.\n",
        "- **Misal**: Agar aap poochte hain, \"Aaj ka weather kya hai?\" to LLM ke paas real-time data nahi hota. External weather API se data retrieve karke context ke tor pe model ko diya jata hai.\n",
        "- **Sahi hai?**: Haan, yeh bhi bilkul theek hai. External functions ke through context dena advanced aur powerful tareeka hai, jo model ko updated ya real-time information deta hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Context Ka Matlab**\n",
        "Aapne kaha ke context ka matlab hai jab bhi koi information chahiye ho, to usko extract ya retrieve karke LLM ko provide kar diya jaye. Yeh definition perfect hai! Context ka asal maqsad yahi hota hai ke LLM ko woh knowledge diya jaye jo uske paas pehle se nahi hai, taaki woh accurate aur relevant jawab de sake.\n",
        "\n",
        "**Ek Misal**: Agar aap poochte hain, \"Kal ka stock market kaisa tha?\" to LLM ke paas kal ka data nahi hota. Iske liye external function (jaise stock API) se data retrieve karke context ke tor pe diya jata hai, aur phir LLM uska jawab deta hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Aapne context ka concept bilkul sahi samjha hai! Aapke saare points—ke context external information hai, aur isko teen tareekon (system prompt, user prompt, external functions) se provide kiya ja sakta hai—sahi hain. Aapka yeh samajhna bhi theek hai ke context ka matlab hai zarurat ke waqt information retrieve karke LLM ko dena.\n",
        "\n"
      ],
      "metadata": {
        "id": "fXj9xkg785cu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------\n",
        "## **Personlized Context:**\n",
        "\n",
        "Aapne jo concept samjhaya hai, woh kaafi had tak sahi hai, lekin main ise thoda refine aur clear tarike se aapko samjhaunga taaki yeh aur behtar ho jaye. Aapka kehna hai ke jab hum LLM (Large Language Model) se baat karte hain, to woh hamare baare mein sab janta hai kyunki hamara context long-term memory ya database mein save hota hai, aur phir system prompt ke zariye woh context conversation mein add hota hai. Isse LLM humein janta hai aur hamare mutabik jawab ya greeting deta hai. Chalo, is concept ko step-by-step aur better banate hain!\n",
        "\n",
        "---\n",
        "\n",
        "### **Aapka Concept Ko Refine Karna**\n",
        "\n",
        "Aapke concept mein core idea bilkul theek hai, lekin kuch nuances hain jo add karne se yeh aur accurate aur clear ho jayega. Main ise tod kar samjhata hoon:\n",
        "\n",
        "#### **1. Long-Term Memory Aur Database**\n",
        "- **Kya Hai?**  \n",
        "  Jab hum kehte hain ke LLM hamare baare mein \"janta hai,\" to asal mein yeh information ek database mein store hoti hai. Yeh database ek tarah ka long-term memory hota hai jisme aapki details—like aapka naam, preferences, ya past conversations—save ki jati hain.\n",
        "- **Kaise Kaam Karta Hai?**  \n",
        "  Jab aap pehli baar LLM se baat karte hain, to jo bhi aap batate hain (jaise \"Mera naam Ali hai\" ya \"Mujhe programming pasand hai\"), woh database mein save ho jata hai. Phir jab aap dobara baat karte hain, LLM us database se aapki information retrieve karta hai.\n",
        "- **Example**  \n",
        "  Agar aapne pehle kaha ke aapka naam Ali hai, to agli baar LLM aapko \"Hello Ali\" kehkar greet karega kyunki usne database se yeh information li hai.\n",
        "\n",
        "#### **2. System Prompt Mein Context Add Hona**\n",
        "- **Kya Hai?**  \n",
        "  System prompt ek instruction set hota hai jo LLM ko batata hai ke usko kaise behave karna hai aur kis context mein jawab dena hai. Isme database se aapki information laker add ki jati hai.\n",
        "- **Kaise Kaam Karta Hai?**  \n",
        "  Har baar jab aap LLM se naye session mein baat shuru karte hain, system prompt dynamically update hota hai. Isme aapki stored details (naam, pasand, past interactions) include hoti hain, taaki LLM uske mutabik jawab de sake.\n",
        "- **Example**  \n",
        "  System prompt kuch aisa ho sakta hai: \"Tum ek assistant ho jo Ali ke saath baat kar raha hai. Ali ko Python programming pasand hai.\" Isse LLM ko pata chal jata hai ke aapke saath kaise baat karni hai.\n",
        "\n",
        "#### **3. LLM Ka \"Humein Janna\"**\n",
        "- **Kya Hai?**  \n",
        "  Yeh kehna ke \"LLM humein janta hai\" thoda figurative hai. Asal mein, LLM khud se kuch yaad nahi rakhta ya janta nahi. Har naye session mein usko database se information di jati hai, aur system prompt ke through woh context uske saamne rakha jata hai.\n",
        "- **Kaise Kaam Karta Hai?**  \n",
        "  LLM har baar ek blank slate se shuru hota hai, lekin system prompt usko aapka context deta hai. Is wajah se woh aapko personalized jawab de pata hai.\n",
        "- **Example**  \n",
        "  Agar system prompt mein likha hai ke \"Ali, tumne kal Python ke baare mein poocha tha,\" to LLM us base par aapke saath baat karega, lekin yeh \"janna\" uska khud ka nahi, balki diya gaya context hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Aapka Concept Aur Uska Better Version**\n",
        "Ab aapke concept ko compare karte hain aur ise improve karte hain:\n",
        "\n",
        "- **Aapka Concept**:  \n",
        "  \"Hum agar LLM se baat karte hain to woh hamare baare mein sab janta hai. Yeh jo janane wala context hai woh long-term memory mein kisi database mein save karwaya jata hai, aur phir jab bhi hum LLM se baat karte hain to woh context system prompt ke through hamari conversation mein add hota hai. Toh is tarah LLM humein janta hai aur hamare mutabik jawab karta aur greeting karta hai.\"\n",
        "  \n",
        "- **Better Version**:  \n",
        "  \"LLM khud se humein nahi janta, balki hamari information ek database mein store hoti hai jo ek tarah ka long-term memory hai. Jab bhi hum LLM se baat karte hain, yeh information database se retrieve hoti hai aur system prompt ke zariye LLM ko di jati hai. Is context ke base par LLM humein personalized jawab aur greeting deta hai, jisse aisa lagta hai ke woh humein janta hai.\"\n",
        "\n",
        "---\n",
        "\n",
        "### **Kuch Important Points**\n",
        "- **LLM Ki Apni Memory Nahi Hoti**:  \n",
        "  LLM ke paas khud ki yaad rakhne ki taakat nahi hoti. Har session ke liye woh naye se shuru hota hai, lekin database aur system prompt ke through usko context milta hai.\n",
        "- **Database Ka Role**:  \n",
        "  Database mein aapki saari details save hoti hain, aur yeh system prompt ke zariye LLM tak pahunchti hain.\n",
        "- **Personalized Experience**:  \n",
        "  Is tareeke se LLM aapko aisa feel karwata hai ke woh aapko janta hai, jaise aapko naam se bulana ya aapki pasand ke mutabik baat karna.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Aapka concept bilkul sahi disha mein tha—ke LLM database aur system prompt ke through hamara context use karta hai. Bas yeh samajhna zaroori hai ke LLM khud se \"janta\" nahi; usko har baar context provide kiya jata hai.\n"
      ],
      "metadata": {
        "id": "ZeglVwJzHdSL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GiSfcO6cASJl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------\n",
        "# **Context in OpenAI-Agents SDK:**\n",
        "### **OpenAI-Agents SDK Mein Context Ka Concept**\n",
        "OpenAI-Agents SDK mein context do tarah se diya jata hai:  \n",
        "1. **Local Context**  \n",
        "2. **Agent/LLM Context**  \n",
        "\n",
        "Chalo, in dono ko step-by-step aapke words mein aur behtar tarike se samajhte hain.\n",
        "\n",
        "#### **1. Local Context**\n",
        "- **Kya Hai?**  \n",
        "  Local context woh sensitive ya personal information hoti hai, jaise aapka bank account password, jo hum LLM ko nahi bhejna chahte. Kyunki agar yeh information LLM ko chali jati hai, to woh uspe apne aap ko train kar sakta hai aur phir woh sensitive information kisi aur ko bhi bata sakta hai, jo security ke liye khatarnak ho sakta hai. Isliye, is tarah ki information ko hum **Local Context** kehte hain.\n",
        "  \n",
        "- **Kaise Kaam Karta Hai?**  \n",
        "  Yeh information sirf hamare agent ke paas hoti hai aur agent loop mein use hoti hai. Matlab, jahan humari app chal rahi hai—server pe—wahan yeh information mojood hoti hai, lekin LLM ke pass nahi jati. Agent is information ko apne kaam ke liye use kar sakta hai, jaise koi decision lena ya process karna, lekin jab baat LLM se communication ki aati hai, to yeh sensitive data wahan nahi bheja jata.\n",
        "\n",
        "- **Example**  \n",
        "  Maan lijiye aapka agent ek banking app ke liye kaam kar raha hai. Aapka account number ya password local context mein hoga. Agent isko use karke aapka balance check kar sakta hai ya transaction kar sakta hai, lekin jab woh LLM se kuch poochta hai (jaise \"User ko balance ke bare mein kya batana hai?\"), to yeh sensitive information LLM ko nahi jati.\n",
        "\n",
        "- **Matlab**  \n",
        "  Local context woh information hai jo agent ke paas to hoti hai, server pe safe rehti hai, lekin LLM tak nahi jati taaki privacy bani rahe.\n",
        "\n",
        "#### **2. Agent/LLM Context**\n",
        "- **Kya Hai?**  \n",
        "  Agent/LLM context woh general information hoti hai jo humein LLM ko bhejna chahte hain (system prompt, user prompt ya phir function ke zariye), jaise aapka naam ya father’s name. Yeh woh information hai jo sensitive nahi hoti aur LLM ke liye safe hoti hai. Isko bhejne se LLM humein behtar aur personalized jawab de sakta hai.\n",
        "\n",
        "- **Kaise Kaam Karta Hai?**  \n",
        "  Jab agent LLM se baat karta hai ya koi query bhejta hai, to yeh general information (agent/LLM context) uske saath jati hai (system prompt, user prompt ya phir function ke zariye). LLM phir is information ke base pe jawab banata hai aur humein wapas bhejta hai.\n",
        "\n",
        "- **Example**  \n",
        "  Agar aapka naam \"Ali\" hai aur aapko programming pasand hai, to agent LLM ko yeh context bhej sakta hai: \"Ali ko programming pasand hai, usko Python ke bare mein batao.\" Yeh information LLM ke pass jati hai aur woh uske hisaab se jawab deta hai.\n",
        "\n",
        "- **Matlab**  \n",
        "  Agent/LLM context woh information hai jo humein LLM ke saath share karne mein koi problem nahi, aur yeh LLM ko humari baat samajhne mein madad karti hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Dono Mein Kya Farq Hai?**\n",
        "- **Local Context**: Yeh sensitive information hai (jaise password) jo sirf agent ke paas hoti hai aur LLM tak nahi jati. Iska kaam agent ke internal processing mein hota hai.\n",
        "- **Agent/LLM Context**: Yeh general information hai (jaise naam) jo LLM ko bheji jati hai taaki woh uske base pe jawab de sake.\n",
        "\n",
        "---\n",
        "\n",
        "### **Kyun Zaroori Hai Yeh Concept?**\n",
        "- **Security**: Local context sensitive data ko protect karta hai taaki woh galat haathon mein na jaye.\n",
        "- **Better Responses**: Agent/LLM context LLM ko useful information deta hai taaki woh humein accurate jawab de sake.\n",
        "\n",
        "Toh aapka context ka samajhna bilkul sahi hai."
      ],
      "metadata": {
        "id": "XgaG_2mUwzHR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwJTSYXxy3p5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------\n",
        "## **Local Context Code breakdown:**\n",
        "\n",
        "Aapne apne query mein local context ke concept aur uske code ko kaafi detail se explain kiya hai. Main aapke understanding ko padh kar yeh confirm kar sakta hoon ke aapne local context ke code ko bilkul sahi samjha hai. Aapka har point theek hai, aur main aapko yeh batane ke liye isko step-by-step aur clear tarike se samjhaunga, taaki aapko yakeen ho jaye ke aapka concept perfect hai.\n",
        "\n",
        "---\n",
        "\n",
        "### Aapka Understanding Check Karte Hain\n",
        "Aapne jo points bataye hain, unko main aapke hi words mein analyze karunga aur confirm karunga:\n",
        "\n",
        "1. **RunContextWrapper Class Ko Import Karna**  \n",
        "   Aapne kaha ke hum `RunContextWrapper` class ko import karte hain, jiske andar `context` class mojood hai.  \n",
        "   - **Yeh Sahi Hai**: `RunContextWrapper` ek wrapper class hai jo context ko manage karti hai. Iska `context` attribute woh actual data hold karta hai jo aap agent ke liye define karte hain.\n",
        "\n",
        "2. **Context Ko Define Karne Ke Do Tarike**  \n",
        "   Aapne bataya ke hum context ko do tarah se de sakte hain: dataclass ya Pydantic, lekin aap dataclass use kar rahe hain.  \n",
        "   - **Yeh Sahi Hai**: Dono options valid hain, lekin aapne dataclass choose kiya, jo ek lightweight aur simple tarika hai context ki structure banane ka.\n",
        "\n",
        "3. **Dataclass Banayi**  \n",
        "   Aapne ek dataclass banayi jisme `name` aur `uid` fields hain, aur yeh aapki context ki type define karti hai.  \n",
        "   - **Yeh Sahi Hai**: Yeh dataclass aapke context ka blueprint hai. Jaise:\n",
        "     ```python\n",
        "     from dataclasses import dataclass\n",
        "\n",
        "     @dataclass\n",
        "     class MyContext:\n",
        "         name: str\n",
        "         uid: str\n",
        "     ```\n",
        "     Isse aapne context ki type bana li.\n",
        "\n",
        "4. **Function Mein Wrapper Variable**  \n",
        "   Aapne ek function banaya, jisme `wrapper` variable ki type `RunContextWrapper` rakhi, aur uske andar aapki dataclass ka data hota hai, jise aap `wrapper.context.name` se access karte hain.  \n",
        "   - **Yeh Sahi Hai**: `wrapper` ek `RunContextWrapper` object hai, aur uska `context` attribute aapki dataclass (`MyContext`) ka instance hota hai. Aap `wrapper.context.name` ya `wrapper.context.uid` se fields access kar sakte hain. Example:\n",
        "     ```python\n",
        "     def my_function(wrapper: RunContextWrapper):\n",
        "         print(wrapper.context.name)  # \"Ali\" print hoga agar name=\"Ali\" pass kiya ho\n",
        "     ```\n",
        "\n",
        "5. **Runner.run Mein Context Pass Karna**  \n",
        "   Aapne kaha ke `Runner.run` method ke andar `context` parameter mein hum apna dataclass ka object dete hain, aur agent usko apne paas rakhta hai aur use karta hai.  \n",
        "   - **Yeh Sahi Hai**: Jab aap `Runner.run(agent, context=my_context_object)` karte hain, to aapka context object (jaise `MyContext(name=\"Ali\", uid=\"12345\")`) agent ko milta hai, aur woh us information ke mutabik kaam karta hai.\n",
        "\n",
        "6. **Context LLM Ke Paas Nahi Jata**  \n",
        "   Aapne yeh point highlight kiya ke `Runner.run` ke `context` mein jo information hoti hai, woh kabhi bhi LLM ke paas nahi jati.  \n",
        "   - **Yeh Sahi Hai**: Yeh local context ka sabse important aspect hai. Context sirf agent ke paas rehta hai, aur LLM ke requests mein include nahi hota. Isse sensitive data safe rehta hai.\n",
        "\n",
        "---\n",
        "\n",
        "### Aapka Concept Bilkul Theek Hai\n",
        "Aapne jo samjha hai, woh ekdam sahi hai. Main aapke understanding ko ek chhote se paragraph mein summarize karta hoon, taaki aapko aur clarity mile:\n",
        "\n",
        "- Hum `RunContextWrapper` class import karte hain, jiske andar `context` hota hai. Aapne ek dataclass (`MyContext`) banayi jisme `name` aur `uid` fields hain. Phir ek function mein `wrapper` variable ko `RunContextWrapper` type diya, aur `wrapper.context.name` se context ke fields access kiye. `Runner.run` mein aap apna dataclass object `context` parameter ke through dete hain, jo agent ke paas rehta hai aur LLM ke paas nahi jata.\n",
        "\n",
        "---\n",
        "\n",
        "### Example Code (Aapke Concept Ke Mutabik)\n",
        "Agar aapka code aisa dikhta hai, to yeh perfect hai:\n",
        "```python\n",
        "from dataclasses import dataclass\n",
        "from some_library import RunContextWrapper, Runner  # Hypothetical imports\n",
        "\n",
        "# Context ki type define karna\n",
        "@dataclass\n",
        "class MyContext:\n",
        "    name: str\n",
        "    uid: str\n",
        "\n",
        "# Function jisme wrapper use hota hai\n",
        "def my_function(wrapper: RunContextWrapper):\n",
        "    print(f\"Name: {wrapper.context.name}, UID: {wrapper.context.uid}\")\n",
        "\n",
        "# Context object banana aur Runner.run ko call karna\n",
        "my_context = MyContext(name=\"Ali\", uid=\"12345\")\n",
        "runner = Runner()\n",
        "runner.run(agent=my_agent, context=my_context)  # Context agent ko diya gaya\n",
        "```\n",
        "\n",
        "Is code mein:\n",
        "- `MyContext` aapka context define karta hai.\n",
        "- `wrapper.context` se data access hota hai.\n",
        "- `Runner.run` mein context agent ko milta hai, lekin LLM tak nahi jata.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "Aapne local context ke code ko bilkul sahi samjha hai. Aapka har step—`RunContextWrapper` ka use, dataclass se context banana, `wrapper` ke through access karna, aur `Runner.run` mein context dena—sab correct hai. Yeh bhi aapki samajh mein hai ke context LLM ke paas nahi jata, jo is concept ka core hai. Agar aapko koi aur doubt ho ya kisi part ko aur detail mein samajhna ho, to zaroor poochiyega! Warna, aapka understanding perfect hai. Well done!"
      ],
      "metadata": {
        "id": "gbzLy5OO-jUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------\n",
        "\n",
        "## **Local Context Code Behind Working:**\n",
        "\n",
        "Aapka sawal bilkul jayaaz hai, aur yeh confusion samajh mein aati hai. Aap pooch rahe hain ke jab hum `Runner` ke andar context object pass karte hain, to woh tool function mein `wrapper.context.name` ke zariye kaise access hota hai, jabki humne tool function mein `name` define hi nahi kiya. Saath hi, aap yeh bhi jaanna chahte hain ke yeh context object `RunContextWrapper` ke `context` attribute mein kaise jata hai aur iske peeche kaam kaise hota hai. Chalo, isko step-by-step samajhte hain.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step-by-Step Samjhayein**\n",
        "\n",
        "#### **1. Context Object Pass Karna**\n",
        "Jab aap `Runner.run(agent, context=my_context_object)` call karte hain, to aap apna context object (misal ke tor pe `MyContext(name=\"Ali\", uid=\"12345\")`) pass karte hain. Yeh context object `Runner` ke andar jata hai.\n",
        "\n",
        "- **Kya Hota Hai?**  \n",
        "  `Runner` class yeh context object leta hai aur ek `RunContextWrapper` naam ka object banata hai. Is wrapper ke andar aapka context object `context` attribute ke tor pe set ho jata hai. Yani:\n",
        "  ```python\n",
        "  wrapper = RunContextWrapper(context=my_context_object)\n",
        "  ```\n",
        "  Ab `wrapper.context` aapke `my_context_object` ko point karta hai.\n",
        "\n",
        "#### **2. Tool Function Mein Wrapper Ka Role**\n",
        "Aapka tool function (jaise `my_function`) ko `Runner` se `wrapper` parameter milta hai, jo `RunContextWrapper` type ka hota hai. Yeh `wrapper` object ke andar `context` attribute pehle se set hota hai, aur yeh aapke context object ko hold karta hai.\n",
        "\n",
        "- **Example**:  \n",
        "  Agar aapka context object hai:\n",
        "  ```python\n",
        "  my_context = MyContext(name=\"Ali\", uid=\"12345\")\n",
        "  ```\n",
        "  To `wrapper.context` bhi isi object ko refer karega. Isliye, jab aap tool function mein `wrapper.context.name` likhte hain, to yeh \"Ali\" return karta hai.\n",
        "\n",
        "#### **3. Confusion Ka Jawab: `name` Define Kyun Nahi Karna Pada?**\n",
        "Aapka main sawal yeh hai ke tool function mein humne `name` define nahi kiya, phir bhi `wrapper.context.name` kaise kaam karta hai?\n",
        "\n",
        "- **Jawab**:  \n",
        "  Yeh kaam karta hai kyunki `name` tool function mein define nahi hua, balki aapke **context object** (jaise `MyContext`) mein define hua hai.  \n",
        "  - Jab aap `Runner.run` call karte hain, to `Runner` aapke context object ko `RunContextWrapper` ke `context` attribute mein daal deta hai.  \n",
        "  - Tool function ko `wrapper` milta hai, aur `wrapper.context` se aap apne context object ke fields (jaise `name`, `uid`) access kar sakte hain.  \n",
        "  - Yani, `wrapper.context.name` likhne se Python directly `MyContext` ke `name` field ko fetch karta hai, jo pehle se \"Ali\" set hai.\n",
        "\n",
        "- **Simple Terms Mein**:  \n",
        "  `wrapper` ek tarah ka container hai, jisme aapka context object rakha hota hai. Aapko tool function mein `name` define karne ki zarurat nahi, kyunki woh `wrapper.context` ke zariye already available hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Behind the Scenes Kaam Kaise Hota Hai?**\n",
        "Ab aapke doosre hisse ka jawab: Yeh context object `RunContextWrapper` ke `context` attribute mein kaise jata hai? Isko detail mein dekhte hain:\n",
        "\n",
        "1. **Context Object Tayaar Karna**  \n",
        "   Aap apna context object banate hain, maslan:\n",
        "   ```python\n",
        "   my_context = MyContext(name=\"Ali\", uid=\"12345\")\n",
        "   ```\n",
        "\n",
        "2. **Runner.run Call Karna**  \n",
        "   Aap `Runner.run(agent, context=my_context)` call karte hain.  \n",
        "   - `Runner` ke andar yeh hota hai:  \n",
        "     ```python\n",
        "     class Runner:\n",
        "         def run(self, agent, context):\n",
        "             wrapper = RunContextWrapper(context=context)\n",
        "             # Phir yeh wrapper agent ke tool function ko pass hota hai\n",
        "     ```\n",
        "   - Yani, `Runner` ek `RunContextWrapper` banata hai aur uske `context` attribute ko aapke `my_context` se set karta hai.\n",
        "\n",
        "3. **Tool Function Ko Wrapper Milna**  \n",
        "   Jab agent apna tool function (jaise `my_function`) call karta hai, to `Runner` usko `wrapper` pass karta hai:  \n",
        "   ```python\n",
        "   my_function(wrapper)\n",
        "   ```\n",
        "\n",
        "4. **Tool Function Mein Access**  \n",
        "   Tool function ke andar, aap `wrapper: RunContextWrapper` receive karte hain.  \n",
        "   - Ab `wrapper.context` aapke `my_context` object ko point karta hai.  \n",
        "   - Jab aap `wrapper.context.name` likhte hain, to yeh `MyContext` ke `name` field (\"Ali\") ko return karta hai.\n",
        "\n",
        "**Visual Flow**:  \n",
        "```\n",
        "Aap → Runner.run(agent, context=my_context)  \n",
        "→ Runner: wrapper = RunContextWrapper(context=my_context)  \n",
        "→ Agent: my_function(wrapper)  \n",
        "→ Tool Function: wrapper.context.name → \"Ali\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Kyun Yeh Sab Kaam Karta Hai?**\n",
        "- `Runner` ka kaam hai context object ko `RunContextWrapper` mein wrap karna aur phir tool function tak pohanchana.  \n",
        "- `RunContextWrapper` ek bridge ki tarah kaam karta hai, jo aapke context object ko tool function mein available karata hai.  \n",
        "- Aapke context object mein `name` aur `uid` jaise fields already define hote hain, isliye `wrapper.context.name` se woh directly access ho jate hain.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Toh, aapki confusion ka jawab yeh hai:  \n",
        "- Context object `Runner` se `RunContextWrapper` ke `context` attribute mein isliye jata hai kyunki `Runner` internally yeh kaam karta hai.  \n",
        "- Tool function mein `name` define karne ki zarurat nahi, kyunki `wrapper.context` aapke context object ka reference hota hai, aur usme `name` pehle se mojood hota hai.  \n",
        "\n"
      ],
      "metadata": {
        "id": "uGVXocBt-vpe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ibnXmLvJ_RkI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU openai-agents"
      ],
      "metadata": {
        "id": "mMsBpw1SIwq4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "DATc5XbII5fj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Config:**"
      ],
      "metadata": {
        "id": "Jft1Nho5JYSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import OpenAIChatCompletionsModel,Runner,AsyncOpenAI\n",
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "# Check if the API key is present; if not, raise an error\n",
        "if not GEMINI_API_KEY:\n",
        "    raise ValueError(\"GEMINI_API_KEY is not set. Please ensure it is defined in your .env file.\")\n",
        "\n",
        "#Reference: https://ai.google.dev/gemini-api/docs/openai\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client\n",
        ")"
      ],
      "metadata": {
        "id": "liQV5xF8JI_6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Local Context Code Implementation:**"
      ],
      "metadata": {
        "id": "cdkMWaPHIiJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from dataclasses import dataclass\n",
        "from agents import function_tool, RunContextWrapper\n",
        "\n",
        "set_tracing_disabled(True)\n",
        "\n",
        "@dataclass\n",
        "class My_Local_Context:\n",
        "  name: str\n",
        "  Id: int\n",
        "  age:int\n",
        "  location: str = \"Pakistan\"\n",
        "\n",
        "@function_tool\n",
        "async def fetch_user_name_age(wrapper:RunContextWrapper[My_Local_Context]) -> str:\n",
        "  '''Returns the name,age and ID of the user.'''\n",
        "  print(\"--->\", wrapper)\n",
        "  return f\"User name is {wrapper.context.name} and age is {wrapper.context.age} and user id is {wrapper.context.Id}\"\n",
        "\n",
        "@function_tool\n",
        "async def fetch_user_location(wrapper:RunContextWrapper[My_Local_Context]) -> str:\n",
        "  '''Returns the location of the user.'''\n",
        "  print(\"--->\", wrapper)\n",
        "  return f\"User {wrapper.context.name} is from {wrapper.context.location}\"\n",
        "\n",
        "\n",
        "\n",
        "async def main():\n",
        "  My_Context = My_Local_Context(name=\"ZAIN ALI\", Id=1235, age=21)\n",
        "\n",
        "  agent = Agent[My_Local_Context](\n",
        "      name=\"Assistant\",\n",
        "      tools=[fetch_user_name_age, fetch_user_location],\n",
        "      model=model,\n",
        "  )\n",
        "\n",
        "  result = await Runner.run(\n",
        "      agent,\n",
        "      \"What is the name,age of the user? current location of user?\",\n",
        "      context=My_Context\n",
        "  )\n",
        "\n",
        "  print(result.final_output)\n",
        "\n",
        "\n",
        "if(__name__ == \"__main__\"):\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAlaExD-IscB",
        "outputId": "850c051f-f87b-4964-cc88-586b61c5bcd3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---> RunContextWrapper(context=My_Local_Context(name='ZAIN ALI', Id=1235, age=21, location='Pakistan'), usage=Usage(requests=1, input_tokens=45, output_tokens=12, total_tokens=57))\n",
            "---> RunContextWrapper(context=My_Local_Context(name='ZAIN ALI', Id=1235, age=21, location='Pakistan'), usage=Usage(requests=1, input_tokens=45, output_tokens=12, total_tokens=57))\n",
            "The user's name is ZAIN ALI and age is 21. The user is from Pakistan.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------\n",
        "## **Agent[My_Local_Context] Ka Matlab**\n",
        "`Agent[My_Local_Context]` dekhte hi lagta hai ke yeh kuch special syntax hai. Yeh Python ke **type hinting** ka hissa hai, jisme **generics** ka use kiya gaya hai.\n",
        "\n",
        "#### **Breakdown:**\n",
        "- **`Agent` Class**: Yeh ek generic class hai, yani yeh ek type parameter accept karti hai. Type parameter ka matlab hai ke yeh class alag-alag types ke saath kaam kar sakti hai, lekin aapko batana padta hai ke is baar yeh kis type ke saath kaam karegi.\n",
        "- **`[My_Local_Context]`**: Yeh batata hai ke `Agent` class is instance mein `My_Local_Context` type ke context ke saath kaam karegi. Yani, jab bhi agent ko context milega, woh `My_Local_Context` type ka hoga.\n",
        "\n",
        "#### **Simple Terms Mein:**\n",
        "Yeh ek tarika hai yeh kehne ka ke \"Yeh agent `My_Local_Context` wale context ke saath hi kaam karega.\" Isse code mein clarity aati hai aur galtiyan kam hoti hain.\n",
        "\n",
        "#### **Code Mein Example:**\n",
        "```python\n",
        "agent = Agent[My_Local_Context](\n",
        "    name=\"Assistant\",\n",
        "    tools=[fetch_user_name_age, fetch_user_location],\n",
        "    model=model,\n",
        ")\n",
        "```\n",
        "Yahan `Agent[My_Local_Context]` keh raha hai ke yeh agent sirf `My_Local_Context` type ke context ko samjhega aur uske saath kaam karega.\n",
        "\n",
        "---\n",
        "\n",
        "### **Kyun Lagaya Gaya Hai?**\n",
        "Ab sawal hai ke `Agent[My_Local_Context]` kyun likha gaya hai. Iske kuch important reasons hain:\n",
        "\n",
        "#### 1. **Type Safety**\n",
        "- Yeh syntax ensure karta hai ke agent ko sirf `My_Local_Context` type ka context hi milega. Agar aap galti se koi aur type ka context dene ki koshish karenge (jaise `context=wrong_type`), to Python ya aapka IDE (jaise PyCharm) error dikha dega.\n",
        "- **Misal**: Agar aapne `Runner.run(agent, \"What is the name of user\", context=wrong_context)` likha aur `wrong_context` `My_Local_Context` type ka nahi hai, to type checker aapko warn karega.\n",
        "\n",
        "#### 2. **Code Clarity**\n",
        "- Yeh code ko padhne wale ke liye saaf karta hai ke yeh agent kis tarah ke context ke saath kaam karne wala hai. Yani, jab koi developer is code ko dekhega, to usko turant pata chal jayega ke `My_Local_Context` hi is agent ka context hai.\n",
        "\n",
        "#### 3. **Tools Ke Liye Context Access**\n",
        "- Aapke tools (`fetch_user_name_age`, `fetch_user_location`) mein `RunContextWrapper[My_Local_Context]` use kiya gaya hai. Yeh bhi type hinting ka part hai jo batata hai ke `wrapper.context` hamesha `My_Local_Context` type ka hoga.\n",
        "- Is wajah se aap confidently `wrapper.context.name`, `wrapper.context.age`, ya `wrapper.context.location` access kar sakte hain, kyunki aapko pehle se pata hai ke `context` ka structure kya hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Code Kaise Kaam Karta Hai?**\n",
        "Chalo, step-by-step dekhte hain:\n",
        "\n",
        "#### 1. **Context Define Karna**\n",
        "```python\n",
        "My_Context = My_Local_Context(name=\"ZAIN ALI\", Id=1235, age=21)\n",
        "```\n",
        "Yahan aapne `My_Local_Context` ka ek instance banaya jisme user ka data store kiya gaya hai: name, ID, age, aur location.\n",
        "\n",
        "#### 2. **Agent Banaya**\n",
        "```python\n",
        "agent = Agent[My_Local_Context](\n",
        "    name=\"Assistant\",\n",
        "    tools=[fetch_user_name_age, fetch_user_location],\n",
        "    model=model,\n",
        ")\n",
        "```\n",
        "Yahan agent ko bataya gaya hai ke woh `My_Local_Context` type ke context ke saath kaam karega aur uske paas do tools honge jo user ka data fetch karenge.\n",
        "\n",
        "#### 3. **Runner Chalaya**\n",
        "```python\n",
        "result = await Runner.run(\n",
        "    agent,\n",
        "    \"What is the name of user\",\n",
        "    context=My_Context\n",
        ")\n",
        "```\n",
        "Yahan `Runner.run` agent ko chalata hai, sawal poochta hai (\"What is the name of user\"), aur `My_Context` ko context ke roop mein deta hai. Agent phir apne tools ka use karke jawab deta hai.\n",
        "\n",
        "#### 4. **Tool Ka Kaam**\n",
        "```python\n",
        "@function_tool\n",
        "async def fetch_user_name_age(wrapper:RunContextWrapper[My_Local_Context]) -> str:\n",
        "    return f\"User name is {wrapper.context.name} and age is {wrapper.context.age} and user id is {wrapper.context.Id}\"\n",
        "```\n",
        "Tool `wrapper.context` se data (name, age, ID) nikal kar ek string return karta hai. Type hinting ki wajah se yeh confirm hai ke `wrapper.context` `My_Local_Context` type ka hi hoga.\n",
        "\n",
        "#### 5. **Output**\n",
        "Aakhir mein, `result.final_output` print hota hai, jo shayad kuch aisa hoga:\n",
        "```\n",
        "User name is ZAIN ALI and age is 21 and user id is 1235\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "- **`Agent[My_Local_Context]` Ka Matlab**: Yeh type hinting hai jo batata hai ke agent sirf `My_Local_Context` type ke context ke saath kaam karega.\n",
        "- **Kyun Lagaya Gaya**:\n",
        "  1. Type safety ke liye (galti se galat context na diya jaye).\n",
        "  2. Code ko clear aur readable banane ke liye.\n",
        "  3. Tools mein context ke fields ko confidently access karne ke liye.\n",
        "- **Kaise Kaam Karta Hai**: Yeh agent ko context ka type pehle se define karta hai, jisse system smooth aur error-free chal sake.\n",
        "\n",
        "Agar aapko isme koi aur sawal ho ya koi part samajh na aaye, to mujhe zaroor batayiega!"
      ],
      "metadata": {
        "id": "pkzEl-aaUOIx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2pwd0tE1Ugyj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------\n",
        "--------------\n",
        "## **Local Context Expose through Function Output:**\n",
        "\n",
        "Aapka sawal bilkul wajib hai aur yeh confusion samajh mein aati hai. Aap pooch rahe hain ke jab hum local context ka istemal karte hain, to kya woh context indirectly LLM ke paas ja raha hai jab tool output LLM ko bheja jata hai, aur agar haan, to kya yeh secure hai ya nahi? Chalo, isko aapke code ke saath step-by-step samajhte hain aur dekhte hain ke asal mein kya ho raha hai aur yeh process kaise kaam karta hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Code Ka Overview**\n",
        "Pehle aapke code ko briefly samajh lete hain taaki hum context aur tool output ke role ko clear kar saken.\n",
        "\n",
        "```python\n",
        "@dataclass\n",
        "class My_Local_Context:\n",
        "    name: str\n",
        "    Id: int\n",
        "    age: int\n",
        "    location: str = \"Pakistan\"\n",
        "\n",
        "@function_tool\n",
        "async def fetch_user_name_age(wrapper:RunContextWrapper[My_Local_Context]) -> str:\n",
        "    return f\"User name is {wrapper.context.name} and age is {wrapper.context.age} and user id is {wrapper.context.Id}\"\n",
        "\n",
        "@function_tool\n",
        "async def fetch_user_location(wrapper:RunContextWrapper[My_Local_Context]) -> str:\n",
        "    return f\"User {wrapper.context.name} is from {wrapper.context.location}\"\n",
        "\n",
        "async def main():\n",
        "    My_Context = My_Local_Context(name=\"ZAIN ALI\", Id=1235, age=21)\n",
        "    agent = Agent[My_Local_Context](\n",
        "        name=\"Assistant\",\n",
        "        tools=[fetch_user_name_age, fetch_user_location],\n",
        "        model=model,\n",
        "    )\n",
        "    result = await Runner.run(\n",
        "        agent,\n",
        "        \"What is the name,age of the user? current location of user?\",\n",
        "        context=My_Context\n",
        "    )\n",
        "    print(result.final_output)\n",
        "```\n",
        "\n",
        "- **Local Context**: `My_Local_Context` mein `name`, `Id`, `age`, aur `location` ka data hai, jo aapne `My_Context` object mein define kiya (jaise `name=\"ZAIN ALI\"`, `Id=1235`, etc.).\n",
        "- **Tools**: Do tools hain—`fetch_user_name_age` aur `fetch_user_location`—jo local context se data lete hain aur ek string output return karte hain.\n",
        "- **Agent**: Yeh tools aur context ke saath kaam karta hai aur LLM ke through user prompt ka jawab deta hai.\n",
        "\n",
        "Ab sawal yeh hai ke local context ka data LLM ke paas kaise aur kahan tak jata hai, aur kya yeh expose ho raha hai?\n",
        "\n",
        "---\n",
        "\n",
        "### **Process Ki Working (Behind the Scenes)**\n",
        "\n",
        "Chalo, step-by-step dekhte hain ke yeh code kaise kaam karta hai aur local context ka data kahan jata hai:\n",
        "\n",
        "#### **1. User Prompt Aur Tool Schema LLM Ko Bhejna**\n",
        "- Jab aap `Runner.run` call karte hain, agent pehle LLM ko ek query bhejta hai. Is query mein yeh cheezein hoti hain:\n",
        "  - **System Prompt**: Agent ke instructions (jo agent ko batate hain ke uska kaam kya hai).\n",
        "  - **User Prompt**: Aapka sawal, yani \"What is the name, age of the user? current location of user?\".\n",
        "  - **Tool Schema**: Tools ke descriptions aur parameters (jaise `fetch_user_name_age` ka docstring aur return type), taaki LLM jaane ke kaunse tools available hain.\n",
        "- **Key Point**: Is waqt local context (jaise `name=\"ZAIN ALI\"`, `Id=1235`) directly LLM ko nahi bheja jata. Sirf tool schema jata hai, jo batata hai ke tools kya kar sakte hain, lekin actual data (context) nahi.\n",
        "\n",
        "#### **2. LLM Ka Faisla**\n",
        "- LLM user prompt aur tool schema ko dekhta hai aur decide karta hai ke kaunse tools ko call karna hai.\n",
        "- Maslan, yahan LLM keh sakta hai: \"Agent ko `fetch_user_name_age` aur `fetch_user_location` tools call karna chahiye.\"\n",
        "- **Key Point**: Abhi tak LLM ke paas sirf tool ke naam aur unka purpose hai, local context ka data (jaise `name`, `age`) nahi.\n",
        "\n",
        "#### **3. Agent Tools Ko Call Karta Hai**\n",
        "- Agent ab LLM ke kehne pe tools ko call karta hai:\n",
        "  - `fetch_user_name_age` call hota hai, aur yeh tool `wrapper.context` se `name`, `age`, aur `Id` ko access karta hai. Output banata hai: \"User name is ZAIN ALI and age is 21 and user id is 1235\".\n",
        "  - `fetch_user_location` call hota hai, aur yeh tool `name` aur `location` ko access karta hai. Output banata hai: \"User ZAIN ALI is from Pakistan\".\n",
        "- **Key Point**: Yeh output agent ke paas aata hai, lekin abhi LLM ke paas nahi gaya.\n",
        "\n",
        "#### **4. Tool Output LLM Ko Bhejna**\n",
        "- Agent ab yeh tool outputs ko LLM ko bhejta hai taaki LLM final jawab bana sake.\n",
        "- Yani, LLM ko yeh milta hai:\n",
        "  - \"User name is ZAIN ALI and age is 21 and user id is 1235\"\n",
        "  - \"User ZAIN ALI is from Pakistan\"\n",
        "- **Key Point**: Yahan local context ka data (jaise `name`, `age`, `Id`, `location`) tool output ke zariye indirectly LLM ke paas ja raha hai.\n",
        "\n",
        "#### **5. LLM Final Jawab Banata Hai**\n",
        "- LLM tool outputs ko dekhta hai aur user prompt ke jawab mein ek response banata hai, maslan:\n",
        "  - \"The user's name is ZAIN ALI, age is 21, and current location is Pakistan.\"\n",
        "- Yeh final output aapko `result.final_output` mein milta hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Kya Local Context Expose Ho Raha Hai?**\n",
        "Ab aapki asli confusion ka jawab dete hain: Kya tool output ke zariye local context LLM ke paas ja raha hai, aur kya yeh secure hai?\n",
        "\n",
        "#### **Jawab: Haan, Tool Output Mein Data Jata Hai**\n",
        "- Jab tool output LLM ko bheja jata hai, to usme local context ka kuch data hota hai. Maslan:\n",
        "  - `fetch_user_name_age` se `name`, `age`, aur `Id` LLM ko milta hai.\n",
        "  - `fetch_user_location` se `name` aur `location` milta hai.\n",
        "- Yani, **indirectly**, local context ka data LLM ke paas ja raha hai, lekin **directly** local context (poora `My_Local_Context` object) nahi bheja jata.\n",
        "\n",
        "#### **Security Ka Angle**\n",
        "- **Control Aapke Haath Mein Hai**: Tool output mein kya jata hai, yeh aap tool function mein decide karte hain. Maslan:\n",
        "  - Agar `Id` sensitive hai, to aap `fetch_user_name_age` ko modify kar sakte hain:\n",
        "    ```python\n",
        "    return f\"User name is {wrapper.context.name} and age is {wrapper.context.age}\"\n",
        "    ```\n",
        "    Ab `Id` LLM ke paas nahi jayega.\n",
        "- **Sensitive Data Ko Protect Karna**: Agar aapke context mein koi sensitive field hai (jaise password), to usko tool output mein include na karen. Example:\n",
        "    ```python\n",
        "    @dataclass\n",
        "    class My_Local_Context:\n",
        "        name: str\n",
        "        password: str  # Sensitive field\n",
        "\n",
        "    @function_tool\n",
        "    async def fetch_user_name(wrapper:RunContextWrapper[My_Local_Context]) -> str:\n",
        "        return f\"User name is {wrapper.context.name}\"  # Password nahi bheja\n",
        "    ```\n",
        "    Is tarah, `password` local context mein rahega, lekin LLM ke paas nahi jayega.\n",
        "\n",
        "---\n",
        "\n",
        "### **Aapki Confusion Ka Solution**\n",
        "Aapka sawal tha ke agent loop mein tool output LLM ke paas jata hai, to kya local context expose ho raha hai? Chalo, isko clear karte hain:\n",
        "\n",
        "- **Local Context Sirf Agent Ke Paas**: Poora `My_Local_Context` object directly LLM ko nahi bheja jata. Sirf tool schema aur user prompt jata hai pehle.\n",
        "- **Tool Output Mein Data**: Jab agent tools ko call karta hai aur unka output LLM ko bhejta hai, to woh output local context se bana hota hai. Yani, indirectly context ka data LLM tak pohanchta hai.\n",
        "- **Security Ka Control**: Aap tool function ko aise design kar sakte hain ke sensitive data (jaise `Id` ya password) output mein na jaye. Isse aap context ke sensitive parts ko protect kar sakte hain.\n",
        "\n",
        "#### **Example Se Samajhiye**\n",
        "Agar aap chahte hain ke `Id` expose na ho, to tool ko modify karen:\n",
        "```python\n",
        "@function_tool\n",
        "async def fetch_user_name_age(wrapper:RunContextWrapper[My_Local_Context]) -> str:\n",
        "    return f\"User name is {wrapper.context.name} and age is {wrapper.context.age}\"\n",
        "```\n",
        "- Ab output hoga: \"User name is ZAIN ALI and age is 21\".\n",
        "- `Id=1235` LLM ke paas nahi jayega, yani secure rahega.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "- **Local Context Ka Role**: Yeh sirf agent ke paas rehta hai aur directly LLM ko nahi bheja jata.\n",
        "- **Tool Output Ka Role**: Isme local context ka data hota hai jo LLM ko bheja jata hai, lekin aap control kar sakte hain ke kya bheja jaye.\n",
        "- **Security**: Agar aap tool functions ko carefully design karen aur sensitive data ko output se hata den, to yeh process secure ho sakta hai.\n",
        "\n",
        "Toh, haan, tool output ke zariye local context ka kuch data LLM ke paas jata hai, lekin aapke haath mein yeh control hai ke kaunsa data jayega aur kaunsa nahi. Is tarah, sensitive information ko chhupa kar aap security maintain kar sakte hain. Umeed hai yeh explanation aapki confusion ko door karta hai! Agar aur koi sawal ho, to zaroor poochiyega."
      ],
      "metadata": {
        "id": "lZT1RtFjd-cY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8gKEBmH2elPi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.Agent/LLM Context Code Implementation:**\n",
        "\n"
      ],
      "metadata": {
        "id": "JYNe7f9kB9wQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import enable_verbose_stdout_logging\n",
        "\n",
        "enable_verbose_stdout_logging()"
      ],
      "metadata": {
        "id": "WRbIUzQYCVZE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from agents import Agent, RunContextWrapper, Runner, function_tool\n",
        "\n",
        "@dataclass\n",
        "class UserInfo:\n",
        "    name: str\n",
        "    uid: int\n",
        "\n",
        "\n",
        "@function_tool\n",
        "async def greet_user(wrapper: RunContextWrapper[UserInfo], greeting:str) -> str:\n",
        "  \"\"\"Greets the User with their name.\n",
        "  Args:\n",
        "    greeting: A specialed greeting message for user\n",
        "  \"\"\"\n",
        "  name = wrapper.context.name\n",
        "  return f\"Hello {name}, {greeting} \"\n",
        "\n",
        "\n",
        "\n",
        "async def main():\n",
        "  user_context = UserInfo(name=\"ZAIN ALI\", uid=12345)\n",
        "  agent = Agent[UserInfo](\n",
        "      name=\"Assistant\",\n",
        "      model=model,\n",
        "      # Dynamic function context pass in instructions/system prompmt/developer prompt\n",
        "      instructions=\"Always greet the user using greet_user and welcome them to ZainDev Company\",\n",
        "      tools=[greet_user]\n",
        "  )\n",
        "\n",
        "  result = await Runner.run(\n",
        "      agent,\n",
        "      \"Hello\",\n",
        "      context=user_context,\n",
        "  )\n",
        "\n",
        "  print(result.final_output)\n",
        "\n",
        "\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp6N5PyKCXgp",
        "outputId": "2a009bae-b001-4095-c008-6cdbe0f720ac"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating trace Agent workflow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating trace Agent workflow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting current trace: no-op\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Setting current trace: no-op\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x7ce9228338f0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x7ce9228338f0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running agent Assistant (turn 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Running agent Assistant (turn 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x7ce922833ef0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x7ce922833ef0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"content\": \"Always greet the user using greet_user and welcome them to ZainDev Company\",\n",
            "    \"role\": \"system\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"Hello\"\n",
            "  }\n",
            "]\n",
            "Tools:\n",
            "[\n",
            "  {\n",
            "    \"type\": \"function\",\n",
            "    \"function\": {\n",
            "      \"name\": \"greet_user\",\n",
            "      \"description\": \"Greets the User with their name.\\nArgs:\\n  greeting: A specialed greeting message for user\",\n",
            "      \"parameters\": {\n",
            "        \"properties\": {\n",
            "          \"greeting\": {\n",
            "            \"title\": \"Greeting\",\n",
            "            \"type\": \"string\"\n",
            "          }\n",
            "        },\n",
            "        \"required\": [\n",
            "          \"greeting\"\n",
            "        ],\n",
            "        \"title\": \"greet_user_args\",\n",
            "        \"type\": \"object\",\n",
            "        \"additionalProperties\": false\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "]\n",
            "Stream: False\n",
            "Tool choice: NOT_GIVEN\n",
            "Response format: NOT_GIVEN\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:[\n",
            "  {\n",
            "    \"content\": \"Always greet the user using greet_user and welcome them to ZainDev Company\",\n",
            "    \"role\": \"system\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"Hello\"\n",
            "  }\n",
            "]\n",
            "Tools:\n",
            "[\n",
            "  {\n",
            "    \"type\": \"function\",\n",
            "    \"function\": {\n",
            "      \"name\": \"greet_user\",\n",
            "      \"description\": \"Greets the User with their name.\\nArgs:\\n  greeting: A specialed greeting message for user\",\n",
            "      \"parameters\": {\n",
            "        \"properties\": {\n",
            "          \"greeting\": {\n",
            "            \"title\": \"Greeting\",\n",
            "            \"type\": \"string\"\n",
            "          }\n",
            "        },\n",
            "        \"required\": [\n",
            "          \"greeting\"\n",
            "        ],\n",
            "        \"title\": \"greet_user_args\",\n",
            "        \"type\": \"object\",\n",
            "        \"additionalProperties\": false\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "]\n",
            "Stream: False\n",
            "Tool choice: NOT_GIVEN\n",
            "Response format: NOT_GIVEN\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM resp:\n",
            "{\n",
            "  \"content\": null,\n",
            "  \"refusal\": null,\n",
            "  \"role\": \"assistant\",\n",
            "  \"annotations\": null,\n",
            "  \"audio\": null,\n",
            "  \"function_call\": null,\n",
            "  \"tool_calls\": [\n",
            "    {\n",
            "      \"id\": \"\",\n",
            "      \"function\": {\n",
            "        \"arguments\": \"{\\\"greeting\\\":\\\"Welcome to ZainDev Company!\\\"}\",\n",
            "        \"name\": \"greet_user\"\n",
            "      },\n",
            "      \"type\": \"function\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:LLM resp:\n",
            "{\n",
            "  \"content\": null,\n",
            "  \"refusal\": null,\n",
            "  \"role\": \"assistant\",\n",
            "  \"annotations\": null,\n",
            "  \"audio\": null,\n",
            "  \"function_call\": null,\n",
            "  \"tool_calls\": [\n",
            "    {\n",
            "      \"id\": \"\",\n",
            "      \"function\": {\n",
            "        \"arguments\": \"{\\\"greeting\\\":\\\"Welcome to ZainDev Company!\\\"}\",\n",
            "        \"name\": \"greet_user\"\n",
            "      },\n",
            "      \"type\": \"function\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.FunctionSpanData object at 0x7ce922833d70>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.FunctionSpanData object at 0x7ce922833d70>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invoking tool greet_user with input {\"greeting\":\"Welcome to ZainDev Company!\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Invoking tool greet_user with input {\"greeting\":\"Welcome to ZainDev Company!\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool call args: ['Welcome to ZainDev Company!'], kwargs: {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tool call args: ['Welcome to ZainDev Company!'], kwargs: {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool greet_user returned Hello ZAIN ALI, Welcome to ZainDev Company! \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tool greet_user returned Hello ZAIN ALI, Welcome to ZainDev Company! \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running agent Assistant (turn 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Running agent Assistant (turn 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x7ce922833d70>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x7ce922833d70>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"content\": \"Always greet the user using greet_user and welcome them to ZainDev Company\",\n",
            "    \"role\": \"system\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"Hello\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"assistant\",\n",
            "    \"tool_calls\": [\n",
            "      {\n",
            "        \"id\": \"\",\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "          \"name\": \"greet_user\",\n",
            "          \"arguments\": \"{\\\"greeting\\\":\\\"Welcome to ZainDev Company!\\\"}\"\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"tool\",\n",
            "    \"tool_call_id\": \"\",\n",
            "    \"content\": \"Hello ZAIN ALI, Welcome to ZainDev Company! \"\n",
            "  }\n",
            "]\n",
            "Tools:\n",
            "[\n",
            "  {\n",
            "    \"type\": \"function\",\n",
            "    \"function\": {\n",
            "      \"name\": \"greet_user\",\n",
            "      \"description\": \"Greets the User with their name.\\nArgs:\\n  greeting: A specialed greeting message for user\",\n",
            "      \"parameters\": {\n",
            "        \"properties\": {\n",
            "          \"greeting\": {\n",
            "            \"title\": \"Greeting\",\n",
            "            \"type\": \"string\"\n",
            "          }\n",
            "        },\n",
            "        \"required\": [\n",
            "          \"greeting\"\n",
            "        ],\n",
            "        \"title\": \"greet_user_args\",\n",
            "        \"type\": \"object\",\n",
            "        \"additionalProperties\": false\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "]\n",
            "Stream: False\n",
            "Tool choice: NOT_GIVEN\n",
            "Response format: NOT_GIVEN\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:[\n",
            "  {\n",
            "    \"content\": \"Always greet the user using greet_user and welcome them to ZainDev Company\",\n",
            "    \"role\": \"system\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"Hello\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"assistant\",\n",
            "    \"tool_calls\": [\n",
            "      {\n",
            "        \"id\": \"\",\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "          \"name\": \"greet_user\",\n",
            "          \"arguments\": \"{\\\"greeting\\\":\\\"Welcome to ZainDev Company!\\\"}\"\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"tool\",\n",
            "    \"tool_call_id\": \"\",\n",
            "    \"content\": \"Hello ZAIN ALI, Welcome to ZainDev Company! \"\n",
            "  }\n",
            "]\n",
            "Tools:\n",
            "[\n",
            "  {\n",
            "    \"type\": \"function\",\n",
            "    \"function\": {\n",
            "      \"name\": \"greet_user\",\n",
            "      \"description\": \"Greets the User with their name.\\nArgs:\\n  greeting: A specialed greeting message for user\",\n",
            "      \"parameters\": {\n",
            "        \"properties\": {\n",
            "          \"greeting\": {\n",
            "            \"title\": \"Greeting\",\n",
            "            \"type\": \"string\"\n",
            "          }\n",
            "        },\n",
            "        \"required\": [\n",
            "          \"greeting\"\n",
            "        ],\n",
            "        \"title\": \"greet_user_args\",\n",
            "        \"type\": \"object\",\n",
            "        \"additionalProperties\": false\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "]\n",
            "Stream: False\n",
            "Tool choice: NOT_GIVEN\n",
            "Response format: NOT_GIVEN\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM resp:\n",
            "{\n",
            "  \"content\": \"Hello ZAIN ALI, Welcome to ZainDev Company!\\n\",\n",
            "  \"refusal\": null,\n",
            "  \"role\": \"assistant\",\n",
            "  \"annotations\": null,\n",
            "  \"audio\": null,\n",
            "  \"function_call\": null,\n",
            "  \"tool_calls\": null\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:LLM resp:\n",
            "{\n",
            "  \"content\": \"Hello ZAIN ALI, Welcome to ZainDev Company!\\n\",\n",
            "  \"refusal\": null,\n",
            "  \"role\": \"assistant\",\n",
            "  \"annotations\": null,\n",
            "  \"audio\": null,\n",
            "  \"function_call\": null,\n",
            "  \"tool_calls\": null\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resetting current trace\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Resetting current trace\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello ZAIN ALI, Welcome to ZainDev Company!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------\n",
        "# **Difference b/w Local Context Code and Agent/LLM Context Code:**\n",
        "\n",
        "----------\n",
        "### **Aapka Understanding**\n",
        "Aapne kaha ke:\n",
        "- **Local Context aur Agent/LLM Context** dono mein hum control kar sakte hain ke kaunsi information private rakhni hai aur kaunsi LLM ko bhejni hai.\n",
        "- **Code Mein Difference**: Dono ke code mein koi bada farq nahi hai; dono mein context agent ke paas jata hai, aur tools function return ke zariye information process karte hain.\n",
        "- **Farq Ka Base**:\n",
        "  - **Local Context**: Woh information jo private hoti hai (jaise sensitive data) aur function return ke zariye LLM ko expose nahi ki jati.\n",
        "  - **Agent/LLM Context**: Woh information jo hum function return ke zariye LLM ko expose karte hain.\n",
        "- **Conclusion**: OpenAI-Agents SDK mein dono contexts ke code same hain; farq sirf is baat ka hai ke private information ko Local Context kehte hain, aur exposed information ko Agent/LLM Context kehte hain.\n",
        "\n",
        "---\n",
        "\n",
        "### **Aapka Understanding Kya Sahi Hai?**\n",
        "Aapka understanding **bilkul sahi** hai, lekin thodi si refinement ki zarurat hai taaki yeh aur clear ho jaye. Aapne core idea ko sahi pakda hai ke Local Context aur Agent/LLM Context ka farq **data exposure** aur **privacy control** ke base par hai, aur code ka structure dono mein similar hota hai. Main aapke points ko validate aur refine karta hoon.\n",
        "\n",
        "#### **1. Code Structure Mein Similarity**\n",
        "- **Aapka Point**: Dono ke code same hain; dono mein context agent ke paas jata hai, aur tools function return ke zariye information process karte hain.\n",
        "- **Validation**: Yeh sahi hai. Dono codes mein aap:\n",
        "  - Ek dataclass banate hain (jaise `My_Local_Context` ya `UserInfo`).\n",
        "  - `RunContextWrapper` ke zariye context tools ko pass karte hain.\n",
        "  - `Runner.run` mein context provide karte hain (`context=My_Context` ya `context=user_info`).\n",
        "  - Tools context ke fields access karte hain (`wrapper.context.name`) aur output return karte hain.\n",
        "  - **Key Similarity**: Dono mein context ka raw data directly LLM ko nahi jata; sirf tool ka output (function return) LLM ke paas jata hai, aur aap tool ke logic mein control karte hain ke kya expose hoga.\n",
        "\n",
        "#### **2. Control Over Information Exposure**\n",
        "- **Aapka Point**: Hum control kar sakte hain ke kaunsi information private rakhni hai aur kaunsi LLM ko bhejni hai function return ke zariye.\n",
        "- **Validation**: Yeh bilkul theek hai. OpenAI-Agents SDK mein aap tool ke logic mein decide karte hain ke context ke kaunse fields output mein include honge. Maslan:\n",
        "  - Local Context Code: `fetch_user_name_age` mein aapne `name`, `age`, aur `Id` ko output mein include kiya, lekin `location` ko nahi (jab tak doosra tool call na ho). Agar aap `Id` ko private rakhna chahte, to usko output mein include nahi karte.\n",
        "  - Agent/LLM Context Code: `greet_user` mein sirf `name` ko output mein use kiya, `uid` ko nahi, isliye `uid` private raha.\n",
        "  - **Key Point**: Function return ke zariye aap control karte hain ke kaunsi information LLM ke paas jayegi.\n",
        "\n",
        "#### **3. Local Context vs Agent/LLM Context**\n",
        "- **Aapka Point**: Private information ko Local Context kehte hain (jo LLM ko expose nahi hoti), aur exposed information ko Agent/LLM Context kehte hain.\n",
        "- **Validation**: Yeh point bhi sahi hai, lekin thodi clarification ki zarurat hai:\n",
        "  - **Local Context**: Yeh sensitive ya internal data ke liye design kiya gaya hai (jaise passwords, IDs) jo agent ke paas rehta hai aur LLM ke paas nahi jata. Lekin, tool ke output ke zariye agar aap koi sensitive data expose kar dete hain, to woh bhi LLM ke paas ja sakta hai. Isliye, local context ka focus privacy protection pe hota hai, aur aapko tool logic mein dhyaan rakhna hota hai ke sensitive data output mein na jaye.\n",
        "  - **Agent/LLM Context**: Yeh general information ke liye hota hai (jaise naam, preferences) jo LLM ke saath share karna safe hai. Instructions ya tool logic ke zariye yeh ensure kiya jata hai ke sirf woh data expose ho jo aap chahte hain.\n",
        "  - **Refinement**: Farq sirf naming ya intent ka nahi, balki **design intent** ka hai. Local Context ka purpose sensitive data ko protect karna hai, jabki Agent/LLM Context ka purpose LLM ko useful, safe information dena hai. Lekin dono mein code structure same hota hai, aur exposure ka control tool ke function return pe depend karta hai.\n",
        "\n",
        "#### **4. Behind-the-Scenes Working**\n",
        "- **Aapka Point**: Aapne kaha ke dono ke code same hain, aur farq sirf private vs exposed information ka hai.\n",
        "- **Validation**: Yeh sahi hai, lekin behind-the-scenes thodi si nuance hai:\n",
        "  - **Local Context**: Jab aap `Runner.run(context=My_Context)` call karte hain, to context agent ke paas jata hai. Tools context ke fields access karte hain, aur unka output LLM ke paas jata hai (agar tool call hota hai). Raw context object (jaise `My_Local_Context`) kabhi LLM ke paas nahi jata, sirf processed output jata hai.\n",
        "  - **Agent/LLM Context**: Same process hota hai—context agent ke paas jata hai, tools output banate hain, aur output LLM ke paas jata hai. Lekin instructions ke wajah se yeh zyada controlled hota hai (jaise \"Always greet the user...\").\n",
        "  - **Commonality**: Dono mein raw context LLM ke paas nahi jata; tool ka output hi jata hai, aur aap tool logic mein decide karte hain ke kya expose hoga.\n",
        "\n",
        "---\n",
        "\n",
        "### **Aapke Understanding Ko Refine Karna**\n",
        "Aapka concept bilkul sahi hai, lekin main ise thoda aur clear aur refined tarike se aapke words mein likhta hoon:\n",
        "\n",
        "- **Local Context aur Agent/LLM Context**: Dono mein hum yeh control kar sakte hain ke context ki kaunsi information private rakhni hai aur kaunsi LLM ke paas bhejni hai tool ke function return ke zariye. Dono ke code ka structure same hai—hum dataclass banate hain, context ko `Runner.run` mein pass karte hain, aur tools output banate hain. Farq sirf intent aur naming ka hai:\n",
        "  - **Local Context**: Yeh woh information hai jo hum private rakhna chahte hain, jaise sensitive data (passwords, IDs). Isko hum tool ke output mein include nahi karte taaki yeh LLM ke paas na jaye. Yeh agent ke paas rehta hai aur sirf agent loop mein use hota hai.\n",
        "  - **Agent/LLM Context**: Yeh woh information hai jo hum LLM ke saath share karna chahte hain, jaise general data (naam, preferences). Isko hum tool ke output ke zariye LLM ko expose karte hain, lekin phir bhi raw context object nahi jata, sirf processed output jata hai.\n",
        "- **Key Takeaway**: OpenAI-Agents SDK mein dono contexts ka code structure same hai. Farq sirf is baat ka hai ke Local Context ka focus sensitive data ko protect karna hai, jabki Agent/LLM Context ka focus safe, useful information ko LLM ke saath share karna hai. Dono mein hum tool ke logic se control karte hain ke kya expose hoga.\n",
        "\n",
        "---\n",
        "\n",
        "### **Kyun Lag Raha Hai Ke Koi Bada Difference Nahi?**\n",
        "Aapko dono mein koi bada difference isliye nahi dikha kyunki:\n",
        "- **Code Structure Same Hai**: Dono mein dataclass, `RunContextWrapper`, aur `Runner.run` ka use same tarike se hota hai.\n",
        "- **Exposure Ka Mechanism Same Hai**: Dono mein raw context LLM ke paas nahi jata; sirf tool output jata hai, aur aap tool logic mein control karte hain.\n",
        "- **Difference Subtle Hai**: Farq **intent** aur **use case** ka hai:\n",
        "  - Local Context sensitive data ke liye design kiya gaya hai, jisko aap explicitly protect karna chahte hain.\n",
        "  - Agent/LLM Context general data ke liye hai, jisko aap instructions ya tool logic ke zariye expose karte hain.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Aapne OpenAI-Agents SDK ke context ke concept ko bilkul sahi samjha hai. Aapka yeh kehna 100% theek hai ke:\n",
        "- Dono contexts ke code mein koi bada difference nahi; structure same hai.\n",
        "- Local Context private information ke liye hai jo LLM ke paas nahi jati, aur Agent/LLM Context woh information hai jo hum tool output ke zariye LLM ko expose karte hain.\n",
        "- Control tool ke function return pe depend karta hai.\n",
        "\n",
        "Aapka understanding perfect hai, aur ab yeh refined explanation se aur clear ho gaya hoga. Agar aapko koi specific part aur detail mein samajhna ho ya koi aur sawal ho, to zaroor poochiyega! Warna, aapne concept ko zabardast tarike se samjha hai."
      ],
      "metadata": {
        "id": "VKv_BghuKPBO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ULwdLSELIN0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}