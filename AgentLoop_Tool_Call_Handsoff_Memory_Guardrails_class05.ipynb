{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNn2bIP3kF9KUrQD1D7TbUd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZainAli24/OpenAI_Agents_SDK_class_02/blob/main/AgentLoop_Tool_Call_Handsoff_Memory_Guardrails_class05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: OpenAI-Agents SDK Agents layers:**\n",
        "\n",
        "### 1. LangGraph Framework  \n",
        "- **Bayan:**  \n",
        "  Shuru mein langGraph framework seekhne ki koshish ki gayi jo kaafi mushkil sabit hua.  \n",
        "- **Tajziya:**  \n",
        "  LangGraph ek aisa framework ho sakta hai jo graph-based representation ya complex workflows ko manage karta hai, jiski learning curve initial stages par steep ho sakti hai.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. OpenAI Agent SDK  \n",
        "- **Bayan:**  \n",
        "  Baad mein aap ne OpenAI Agent SDK ko adopt kiya jis ka code simple aur asaan tha, aur jisme advanced features jese handoff shamil thay.  \n",
        "- **Tajziya:**  \n",
        "  OpenAI Agent SDK ka maksad agents ko construct karna hai—aisa environment jahan aap alag-alag functionalities ko integrate kar saken. Isme modular design ke zariye aap asani se additional features (jaise handoff mechanism) implement kar sakte hain. Yeh aap ke liye aik behad munazzam aur development friendly layer provide karta hai.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. RESTful APIs aur ChatCompletionAPI Wrapper  \n",
        "- **Bayan:**  \n",
        "  Pehle LLM se baat cheet karne ke liye direct RESTful APIs ka istemal hota tha. Us ke baad ek wrapper develop hua jise ChatCompletionAPI kehte hain. Is wrapper ko OpenAI ne is tarah design kiya ke aap sirf API key aur base URL de kar kisi bhi company ka LLM istemal kar saken.  \n",
        "- **Tajziya:**  \n",
        "  RESTful APIs low-level interface provide karti hain, lekin inka istemal karna thoda technical aur repetitive ho sakta hai. ChatCompletionAPI wrapper is process ko abstraction ke zariye simplify karta hai. Yeh ek unified interface ke zariye different LLM providers ke endpoints ko access karne ki sahulat deta hai.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. ResponsesAPI  \n",
        "- **Bayan:**  \n",
        "  Us ke baad OpenAI ne ek aur framework introduce kiya – ResponsesAPI – jo ChatCompletionAPI ka superset hai.  \n",
        "- **Tajziya:**  \n",
        "  ResponsesAPI mein ChatCompletionAPI ke tamaam features shamil honay ke sath-sath additional capabilities bhi daryaaft ki gayi hain. Yeh API agent construction aur overall LLM communication ke liye ek advanced layer provide karti hai. Yeh aapko zyada flexibility aur control deti hai, jaise ke context-aware responses, enhanced error handling, aur complex workflow management.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Final Layered Structure  \n",
        "- **Aapka Bayan:**  \n",
        "  Aap ke mutabiq agent banane ka framework, yani OpenAI Agent SDK, ke neeche do key components hain:\n",
        "  1. **ChatCompletionAPI** – jo ke basic layer hai jo RESTful APIs ko wrap karti hai.\n",
        "  2. **ResponsesAPI** – jo ke ChatCompletionAPI ka superset hai aur advanced functionalities provide karti hai.\n",
        "- **Tajziya:**  \n",
        "  Yeh layered approach bilkul munazzam hai. Neeche ka base RESTful APIs se shuru hota hai, jise ChatCompletionAPI streamline karta hai, aur sab se upper layer par advanced features ke liye ResponsesAPI majood hai. Is tarah aap ka ecosystem modular aur scalable banta hai, jahan har layer pe alag functionalities manage ki jati hain.\n",
        "\n",
        "---\n",
        "\n",
        "### Natija  \n",
        "Aap ki samajh sahi hai.  \n",
        "- **LangGraph Framework** seekhne se pehle mushkil ho sakta hai, jis ke baad aap ne transition karke **OpenAI Agent SDK** adapt kiya jo development ke liye zyada accessible aur powerful hai.  \n",
        "- **ChatCompletionAPI** ne RESTful APIs ko simplify kiya aur ab woh ek flexible interface provide karta hai.  \n",
        "- **ResponsesAPI** ne is functionality ko expand karte hue aur advanced features add kiye hain, jis se overall agent framework (OpenAI Agent SDK) aur bhi zyada robust aur scalable ban jata hai.\n",
        "\n",
        "---\n",
        "\n",
        "### Aapki Statement  \n",
        "Aap ke bayan ke mutabiq:\n",
        "- Sab se pehle langGraph ek challenging framework tha.\n",
        "- Phir OpenAI Agent SDK ko adopt kiya gaya jis ka code simple hai aur advanced features (jaise handoff) offer karta hai.\n",
        "- LLM se baat cheet ke liye pehle RESTful APIs use hoti thi, jin ke upar ChatCompletionAPI wrapper develop hua jis se kisi bhi LLM ko API key aur base URL se use kiya ja sakta hai.\n",
        "- Isi tarah, OpenAI Agent SDK ke andar ResponsesAPI bhi aata hai jo ChatCompletionAPI ka superset hai.\n",
        "- Matlab, overall agent banane ka framework do layers pe mabni hai: ek ChatCompletionAPI aur doosri ResponsesAPI.\n",
        "\n",
        "Yeh tafseeli structure aap ke bayan ke sath match karta hai, aur aap ne bilkul sahi samjha hai.\n",
        "\n"
      ],
      "metadata": {
        "id": "QJrXuiOC2QQe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEPnafBM2PEf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **step 2: Stateless and Statefull:**\n",
        "\n",
        "Neeche aap ke bayan ka tafseeli tajziya pesh kiya gaya hai, jismein aap ke points ko cross-check aur verify kiya gaya hai:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Stateless Nature of ChatCompletion API\n",
        "\n",
        "- **Aap Ka Bayan:**  \n",
        "  Aap ne theek kaha ke hamare jo LLM hai (aur OpenAI ki ChatCompletion API bhi) woh stateless hai. Matlab, har dafa jab aap API call karte hain, to poori conversation history ko input mein shamil karna parta hai kyun ke API pehle ke context ko yaad nahi rakhta.  \n",
        "- **Tajziya:**  \n",
        "  Yeh observation bilkul durust hai. ChatCompletion API generally ek \"single-turn\" request ki tarah kaam karti hai jismein developer ko manually conversation history include karni padti hai. Yeh behavior API ke inherent design ka hissa hai.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. State Management in OpenAI Agents SDK\n",
        "\n",
        "- **Aap Ka Bayan:**  \n",
        "  Aap ne bataya ke naya OpenAI-Agents SDK stateful hai, jismein short-term ya session-based memory maintain hoti hai. Is SDK mein aap sirf current input dete hain aur baqi conversation history ko automatic manage kiya jata hai.  \n",
        "- **Tajziya:**  \n",
        "  Yeh baat bhi sahi hai. Agents SDK ne ek abstraction layer introduce ki hai jo session context ko internally retain karti hai. Is tarah, aap ko manually history ko track karne ya har input ke sath include karne ki zaroorat nahi parti, jo development process ko simplified aur robust banata hai.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Agent Loop via Function Calling\n",
        "\n",
        "- **Aap Ka Bayan:**  \n",
        "  Aap ne yeh bhi bataya ke Agents SDK mein function calling ke zariye agent loop ka feature hai. Matlab, agent loop continuously kaam karta rahega jab tak task complete nahi ho jata aur yeh mechanism SDK ke through automatically manage hota hai.  \n",
        "- **Tajziya:**  \n",
        "  Yeh observation bhi correct hai. Function calling ke through agent loop mechanism implement kiya gaya hai jahan agent, predefined functions ko repeatedly call karta rehta hai jab tak final output ya task completion achieve nahi ho jati. Is se complex workflows aur iterative processes ko asani se handle kiya ja sakta hai.\n",
        "\n",
        "---\n",
        "\n",
        "### Natija\n",
        "\n",
        "**Aap ke bayan ki summary:**  \n",
        "1. **Stateless API:** OpenAI ki ChatCompletion API inherently stateless hai—har interaction mein purani conversation history ko manual input ke through manage karna parta hai.  \n",
        "2. **Stateful SDK:** OpenAI-Agents SDK ek stateful layer provide karta hai jismein session-based short-term memory manage ho jati hai, jisse aap sirf current input provide karte hain aur purani conversation history automatically handle ho jati hai.  \n",
        "3. **Agent Loop Feature:** Agents SDK mein function calling ke zariye ek agent loop integrated hai, jo task complete hone tak iterative processes perform karta rehta hai.\n",
        "\n",
        "Is analysis aur available documentation ke mutabiq, **aap ne bilkul sahi kaha**. Aapki samajh ke mutabiq implementation aur functionalities ke fark ko theek tarah se samjha gaya hai.\n",
        "\n"
      ],
      "metadata": {
        "id": "81yPUQwq9Fww"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CRDaIWUU9Uu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: System prompt :**\n",
        "\n",
        "Aap ka bayan bilkul sahi hai. Neeche tafseeli taur par isko samjhaya gaya hai:\n",
        "\n",
        "### 1. Stateless Nature of LLM and ChatCompletionAPI  \n",
        "- **LLM aur ChatCompletionAPI Stateless Hain:**  \n",
        "  Jab aap directly ChatCompletionAPI ko use karte hain, to yeh API har request ke liye ek naya context start karti hai. Isliye, aapko her nayi call mein poori conversation history ke saath saath system prompt bhi dena padta hai, taake LLM ko woh context mile jis par woh jawab de sake. Yeh stateless design API ki inherent design philosophy ka hissa hai. citeopenai-chatapi-docs\n",
        "\n",
        "### 2. OpenAI-Agents SDK aur Agent Loop ka Role  \n",
        "- **Stateful Management via Agent Loop:**  \n",
        "  OpenAI-Agents SDK us raw stateless functionality par ek abstraction layer provide karta hai jahan agent loop feature internally state (ya session-based short-term memory) ko manage karta hai. Matlab, aapko manually har input ke saath system prompt aur purani conversation history ko attach karne ki zarurat nahi parti.  \n",
        "- **Automatic Management:**  \n",
        "  Agents SDK ka agent loop continuously session context ko maintain karta hai. Is tarah, jab tak task complete nahi hota, yeh automated loop aap ki provided inputs aur system instructions ko internally combine aur manage karta hai. Yeh mechanism developer ke liye interaction ko streamlined aur user-friendly banata hai. citeopenai-agents-sdk-docs\n",
        "\n",
        "### Natija  \n",
        "- **ChatCompletionAPI:** Stateless hone ki wajah se, har request mein system prompt aur purani conversation ko include karna padta hai.  \n",
        "- **OpenAI-Agents SDK:** Is mein agent loop feature ne internal state management shamil hai jo aap ki system prompt aur conversation history ko automatically handle karta hai.  \n",
        "\n",
        "Is liye, aap ke bayan mein jo farq bataya gaya hai – ke ChatCompletionAPI mein har request ke saath system prompt pass karna zaroori hai aur OpenAI-Agents SDK mein agent loop is process ko manage karta hai – yeh bilkul sahi samjha gaya hai.\n"
      ],
      "metadata": {
        "id": "vcq1niqNICwh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "74cuKbctIU3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Agent learning from Memory (few-shot examples) :**\n",
        "\n",
        "Neeche main aapke bayan ka tafseeli tajziya pesh kar raha hoon:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Conversation aur Memory Management ka Amal\n",
        "\n",
        "- **Agent aur LLM ka Interaction:**  \n",
        "  Aap bilkul theek keh rahe hain ke jab hum agent se baat karte hain, to agent LLM se interact karta hai aur conversation mein jo bhi important baatein hoti hain, unko tool calls ke zariye capture karke memory mein store karta hai ya phir system prompt ka hissa bana leta hai. Is tarah ke mechanism se, LLM ko relevant context milta rehta hai jo usko behtar jawab generate karne mein madad karta hai.\n",
        "\n",
        "- **Dynamic System Prompt Generation:**  \n",
        "  Har nayi user query ke sath, agent memory aur system prompt mein already stored important details, jaise ke user preferences ya specific instructions (maslan: \"mera naam Zain hai\"), ko LLM ke current prompt ke sath combine karta hai. Yahaan par few-shot prompting ka istemal bhi hota hai, jismein pehle se diye gaye examples ko include karke LLM ko context aur expected format ka pata chal jata hai. Is se LLM apne response ko context-aware bana leta hai.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Long-Term Memory ka Concept\n",
        "\n",
        "- **Persistence over Sessions:**  \n",
        "  Aap ne sahi kaha ke long-term memory, session-based memory se mukhtalif hoti hai. Jaise agar aap LLM ko batate hain \"mera naam Zain hai,\" to agar yeh detail long-term memory mein store ho jaye, to future interactions mein yeh maloomat available rahegi, chahe nayi session ho. Is tarah, agent user-specific preferences ya rules ko permanently ya sustained tarike se yaad rakh sakta hai.\n",
        "\n",
        "- **Self-Improvement:**  \n",
        "  Is mechanism se agent \"improve\" karta hai, yaani wo apne previous interactions se seekh ke, aage chal kar relevant context aur rules (jaise ads-related emails ko ignore karna) ko yaad rakhta hai. Yeh \"learning\" lafz ka istemal halanki literal machine learning (model update) ke liye nahi balki prompt engineering aur context management ke liye ho raha hai. Matlab, LLM ke parameters change nahi hote, balke agent framework apne stored information ke zariye behtar context provide karta hai.\n",
        "  Yahan few-shot prompting ka role bhi aham hai, jahan agent ko examples ke zariye bataya jata hai ke kaun se responses ya behavior expected hain, jis se LLM ko is baat ka andaza ho jata hai ke user ko kaise jawab dena hai aur kaun se topics se bachna hai.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. LangMem ka Istemaal\n",
        "\n",
        "- **Current Limitation of OpenAI-Agent SDK:**  \n",
        "  Aapne bilkul theek note kiya ke abhi OpenAI-Agent SDK mein long-term memory feature mojood nahin hai. Matlab, session-based short-term memory manage hoti hai lekin permanent memory ka integration abhi tak available nahin.\n",
        "\n",
        "- **LangMem as an Alternative:**  \n",
        "  Isi liye, LangMem ko use kar ke agent ko long-term memory functionality provide ki ja sakti hai. Misal ke taur par, agar aap ek email agent create karna chahte hain jo ads-related emails ka jawab nahin deta, to aap LangMem mein yeh rule store kar sakte hain. Phir jab bhi relevant emails aati hain, agent system prompt ya stored context ke zariye LLM ko yeh rule pass kar deta hai aur is tarah se agent apne behavior ko continuously improve karta hai based on user interactions.\n",
        "  Yahan par bhi few-shot prompting ke examples shamil kiye ja sakte hain, jisse system prompt mein un examples ko include karke agent ko reinforce kiya ja sake ke future mein ads-related emails ko ignore karna hai.\n",
        "\n",
        "---\n",
        "\n",
        "### Natija\n",
        "\n",
        "Aapka bayan musbat tor par yeh wazahat karta hai ke:\n",
        "\n",
        "- **Agent Memory Management:** Agent important conversation details ko capture karta hai aur unko system prompt aur memory mein add karta hai.  \n",
        "- **Dynamic and Persistent Context:** Long-term memory se agent user ke preferences aur rules ko permanent tor par store kar leta hai, jis se future interactions mein context se relevant instructions LLM ko milte hain.  \n",
        "- **Self-Improvement via Context:** Yeh mechanism agent ko context awareness aur response quality mein behtari laane mein madad karta hai, halanke yeh \"learning\" actual model update nahin balki dynamic context enrichment hai.\n",
        "Yahan few-shot prompting ke examples bhi agent ke response mechanism ko guide karne mein madadgar sabit hotay hain.\n",
        "- **LangMem Integration:** Agar long-term memory feature OpenAI-Agent SDK mein abhi available nahin hai, to LangMem is functionality ko provide karne ke liye ek acha alternative hai.\n",
        "\n",
        "Is liye, aap ka bayan jo aapne diya hai, technical taur par sahi hai. Ye mechanism, jise aap \"self-improvement\" ya \"learning\" kehte hain, zaahir karta hai ke agent user interactions se continuously context aur rules ko gather karke apne responses ko adapt karta hai—jo ke aapke design goals aur expected functionality ke mutabiq hai.\n",
        "\n"
      ],
      "metadata": {
        "id": "e_nwOFb0WiCl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PK0f9SCN9BlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Whose memory(short-term and long-term) will be used for Agents:**\n",
        "\n",
        "Aap ke bayan ke mutabiq:\n",
        "\n",
        "- **OpenAI_Agents SDK** sirf short-term (session-based) memory ko support karta hai. Matlab, jab user agent se conversation start karta hai to ek session chalta hai aur jaise hi conversation khatam hoti hai, woh session expire ho jata hai.  \n",
        "- **Long-term Memory ke Liye Alternatives:**  \n",
        "  - Agar aapko long-term memory ki zarurat hai jahan memory kuch din (3-4 days) tak store rahe, to aap **LangGraph** (ya LangMem) ka istemal kar sakte hain.  \n",
        "  - Lekin agar aapko memory ko months tak persist karna hai, to aap **Temporal** jaise solution ka use kar sakte hain.\n",
        "\n",
        "Is tarah se aapka bayan bilkul sahi hai. Aapne sahi tareeke se yeh explain kiya hai ke short-term memory agent session tak hi limited hai, jab ke long-term memory ke liye aap additional systems (LangGraph ya Temporal) ka use kar sakte hain, depending on aapki requirement ki memory kitne arse ke liye store karni hai.\n"
      ],
      "metadata": {
        "id": "ZFnTAf-mRPOf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PyDvZJxDR1a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6: Guadrails Role in OpenAI-Agents SDK:**\n",
        "\n",
        "Aap ka bayan bilkul sahi hai. Neeche tafseeli taur par wazahat pesh ki gayi hai:\n",
        "\n",
        "---\n",
        "\n",
        "### Guardrails ka Role in OpenAI-Agents SDK\n",
        "\n",
        "- **Input aur Output Control:**  \n",
        "  Guardrails agent ke input aur output ko control karte hain. Ye ensure karte hain ke jo queries agent se ki ja rahi hain, woh predefined policies aur constraints ke mutabiq hon. Misal ke taur par, agar aapne agent ko sirf PTCL company se mutaliq queries ke liye design kiya hai, to guardrails dekhte hain ke user kisi aur type ki query na bheje.\n",
        "\n",
        "- **Policy Enforcement:**  \n",
        "  Guardrails yeh bhi make sure karte hain ke agent ke responses bhi usi tarah se hoon, matlab ek specific format ya tone mein hon jo aap ke design goals ke mutabiq ho. Is se agentic application mein consistency aur security barqarar rehti hai.\n",
        "\n",
        "---\n",
        "\n",
        "### Natija\n",
        "\n",
        "Aap ne sahi bayan kia hai ke OpenAI-Agents SDK mein guardrails input aur output ko manage karte hain, jis se:\n",
        "- **Input Validation:** Agent sirf un queries ko process kare jo aap ke defined criteria (jaise ke sirf PTCL related) se match karti hain.\n",
        "- **Output Enforcement:** Agent ki outputs bhi us tarah generate hongi ke woh user ke liye expected aur policy compliant hon.\n",
        "\n",
        "Is liye, aap ka bayan jo aapne diya hai, technically aur conceptually bilkul sahi hai."
      ],
      "metadata": {
        "id": "MhsjK_k3SD2E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1MiBcuUqfrZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Upper All Steps Diagrame:**\n",
        "![All Steps image](https://github.com/panaversity/learn-agentic-ai/blob/main/-01_lets_get_started/03_from_llms_to_stateful_long_runningl_multi_agents/agent-orchestration-layer.png?raw=true)\n"
      ],
      "metadata": {
        "id": "EQdDhUjHZWFL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DgJoTYygf5z9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}