{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMjq9izyBXHcqhOqLKIQwHE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZainAli24/OpenAI_Agents_SDK_class_02/blob/main/Use_LiteLLM_with_OpenAI_Agent_SDK%2C_Basic_Agent%2C_Agent_with_Tool%2C_Agent_Loop%2C_Handoffs_Class_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msiOl15o69o4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. !pip install -Uq openai-agents \"openai-agents[litellm]\" breakdown:**\n",
        "\n",
        "Chaliye, is command ko break down karke concise tareeke se samjhaata hoon:\n",
        "\n",
        "```\n",
        "!pip install -Uq openai-agents \"openai-agents[litellm]\"\n",
        "```\n",
        "\n",
        "### **Breakdown**\n",
        "Yeh ek pip command hai jo Python packages install karta hai. Isko parts mein dekhte hain:\n",
        "\n",
        "1. **`!`**:\n",
        "   - Yeh Jupyter Notebook ya IPython environment mein use hota hai taake shell command (jaise `pip`) notebook ke andar chala sakein.\n",
        "   - Agar aap normal terminal mein hain, to `!` nahi likhenge.\n",
        "\n",
        "2. **`pip install`**:\n",
        "   - Yeh Python ke package manager `pip` ka command hai jo packages ko install karta hai.\n",
        "\n",
        "3. **`-U`**:\n",
        "   - Yeh flag ka matlab hai **\"upgrade\"**. Agar `openai-agents` package pehle se install hai, to yeh usko latest version mein update kar dega.\n",
        "\n",
        "4. **`-q`**:\n",
        "   - Yeh flag ka matlab hai **\"quiet\"**. Iska kaam hai installation ke output ko kam karna, taake terminal mein zyada logs na dikhein. Yani, chup-chaap install ho jayega.\n",
        "\n",
        "5. **`openai-agents`**:\n",
        "   - Yeh pehla package hai jo install ho raha hai. `openai-agents` ek SDK (software development kit) hai jo OpenAI ke tools ya agents ke saath kaam karne ke liye banaya gaya hai.\n",
        "\n",
        "6. **`\"openai-agents[litellm]\"`**:\n",
        "   - Yeh doosra package ya configuration hai jo install ho raha hai.\n",
        "   - `[litellm]` ka matlab hai ke `openai-agents` ke saath ek optional dependency ya extra feature install ho raha hai, jo `litellm` naam ke package se related hai.\n",
        "   - `litellm` ek lightweight library ho sakti hai jo `openai-agents` ke saath kaam karti hai, maslan, model integration ya API calls ke liye.\n",
        "\n",
        "### **Concise Samajh**\n",
        "- **Pehli Baat**: `!pip install -Uq openai-agents` ne `openai-agents` SDK ko install ya update kiya, quietly (bina zyada output ke).\n",
        "- **Doosri Baat**: `\"openai-agents[litellm]\"` ne `openai-agents` ke saath `litellm` naam ka extra feature ya dependency bhi install kiya, jo is SDK ke kuch specific functionalities ke liye zaroori hai.\n",
        "\n",
        "Bas itna hi! Yeh command do packages ko ek saath set up karta hai taake aap `openai-agents` aur uske `litellm` features ka use kar sakein."
      ],
      "metadata": {
        "id": "j89i88sx7Ahj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Install openai-agents SDK with Litellm:**"
      ],
      "metadata": {
        "id": "lTDa9HBa7UsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq openai-agents  \"openai-agents[litellm]\"\n"
      ],
      "metadata": {
        "id": "SF06hzU27QSb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Make your Jupyter Notebook capable of running asynchronous functions:**"
      ],
      "metadata": {
        "id": "wNKPx5lE7njw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "VNrYUamj7sh-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Run Google Gemini with LiteLLm and OPENAI-Agent SDK:**"
      ],
      "metadata": {
        "id": "718GfqS970Hl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Function Run Sync:**"
      ],
      "metadata": {
        "id": "LtdxzVKR75V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import Agent, Runner, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from google.colab import userdata\n",
        "\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "\n",
        "def starter(model, api_key):\n",
        "\n",
        "  agent = Agent(\n",
        "      name=\"Helper\",\n",
        "      instructions=\"You are Helpful Assistant in AI Cars Researcher\",\n",
        "      model=LitellmModel(model=model, api_key=api_key)\n",
        "  )\n",
        "\n",
        "  result= Runner.run_sync(    # Run AgentLoop\n",
        "      agent,\n",
        "      \"Which Company acheive fully make autonomus car\"\n",
        "  )\n",
        "  print(result.final_output)\n",
        "\n",
        "\n",
        "starter(model=MODEL, api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwQBnD2D7-q5",
        "outputId": "10dc1605-3ce3-449c-89e4-25ae49864b61"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As of my last knowledge update in early 2023, no company has *fully* achieved a completely autonomous car, meaning a Level 5 autonomous vehicle that can drive anywhere, anytime, in any conditions, without any human intervention.\n",
            "\n",
            "However, several companies are making significant progress and have deployed vehicles with advanced autonomous capabilities:\n",
            "\n",
            "*   **Waymo (Alphabet):** Waymo is widely considered a leader in autonomous driving technology. They operate a ride-hailing service called Waymo One in limited areas where their vehicles can drive without a safety driver in the car. However, these areas are geofenced and well-mapped, and the vehicles still encounter limitations in certain conditions.\n",
            "\n",
            "*   **Tesla:** Tesla has deployed its \"Full Self-Driving\" (FSD) Beta to a limited number of users. While Tesla calls it FSD, it is not truly fully autonomous. It is a Level 2 system requiring active driver supervision, and it has faced scrutiny and regulatory investigations due to safety concerns and limitations.\n",
            "\n",
            "*   **Cruise (General Motors):** Cruise has also been testing and deploying autonomous vehicles in specific areas.\n",
            "\n",
            "*   **Other companies:** Several other companies are actively developing autonomous vehicle technology, including Argo AI (though Ford and VW shut down Argo AI), Aurora, and Zoox (Amazon).\n",
            "\n",
            "**Key Considerations:**\n",
            "\n",
            "*   **Level of Autonomy:** The Society of Automotive Engineers (SAE) defines six levels of driving automation, from 0 (no automation) to 5 (full automation). Most systems currently available to the public are Level 2 or, in limited cases, Level 3.\n",
            "*   **Geofencing:** Many autonomous systems are restricted to specific geographic areas (geofenced) where they have been extensively tested and mapped.\n",
            "*   **Weather and Conditions:** Adverse weather conditions, such as heavy rain or snow, can significantly impact the performance of autonomous systems.\n",
            "*   **Regulatory Approval:** Regulatory frameworks for autonomous vehicles are still evolving, and companies face challenges in obtaining approvals for deployment.\n",
            "\n",
            "**In summary,** while several companies have made significant strides in autonomous driving, a true Level 5 autonomous vehicle that can handle all driving tasks in all conditions is not yet a reality. The timeline for achieving full autonomy remains uncertain.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Use tool and Run Async:**\n",
        "\n"
      ],
      "metadata": {
        "id": "DU_apcG9p-vW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Is mien hum ne do ceezay sekhi hai:**\n",
        "----------\n",
        "### **1. Tools yani function calling kese integret kar sakhte hai**\n",
        "----------\n",
        "### **2. Seond pe hume ne dekha ke jo AgentLoop hai woh baar baar LLM ko call kar rha hai aur reasoning engine ke taur par Agent loop LLM ko use kar rha hai:**\n",
        "### is mien jab hum ne Runner se agent loop ko run kia toh agent query le ke aur jo us ko tool provide kia tha wo leke LLM ke pass gia , LLM ne query ko dekhte howe agent se bola ke ke ye tool call karke lao, aur phir agent ne wo tool call kia aur us tool ka answer wapis LLM ke pass le aya aur pehle ki sari conversation bhi agent LLM ke pass ke aya , phir LLM ne tool ke answer aur pehle ki conversation dekhte howe better way mein user ki query ka answer dia hai.\n",
        "\n",
        "--------"
      ],
      "metadata": {
        "id": "MlguTLEC0LaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Runner, Agent, function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from google.colab import userdata\n",
        "import asyncio\n",
        "\n",
        "from agents import enable_verbose_stdout_logging\n",
        "enable_verbose_stdout_logging()\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "@function_tool\n",
        "def wheather(city: str):\n",
        "  print(f\"\\n Getting wheather for {city} \\n\\n\")\n",
        "  return f\"The wheather in {city} is sunny and warm\"\n",
        "\n",
        "\n",
        "async def tooler(model, api_key):\n",
        "  agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You only respond in haikus.\",\n",
        "      model= LitellmModel(model=model, api_key=api_key),\n",
        "      tools=[wheather],\n",
        "  )\n",
        "\n",
        "  result = await Runner.run(\n",
        "      agent,\n",
        "      \"What`s the wheather of Karachi\"\n",
        "  )\n",
        "\n",
        "  print(result.final_output)\n",
        "\n",
        "\n",
        "asyncio.run(tooler(model=MODEL, api_key=GEMINI_API_KEY))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKnYVwpZqSTN",
        "outputId": "136df48f-f238-49b0-ff54-d330e95eaca3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating trace Agent workflow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating trace Agent workflow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting current trace: no-op\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Setting current trace: no-op\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x78130c2f8ad0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x78130c2f8ad0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running agent Assistant (turn 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Running agent Assistant (turn 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x78130c2f8e90>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x78130c2f8e90>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling Litellm model: gemini/gemini-2.0-flash\n",
            "[\n",
            "  {\n",
            "    \"content\": \"You only respond in haikus.\",\n",
            "    \"role\": \"system\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"What`s the wheather of Karachi\"\n",
            "  }\n",
            "]\n",
            "Tools:\n",
            "[\n",
            "  {\n",
            "    \"type\": \"function\",\n",
            "    \"function\": {\n",
            "      \"name\": \"wheather\",\n",
            "      \"description\": \"\",\n",
            "      \"parameters\": {\n",
            "        \"properties\": {\n",
            "          \"city\": {\n",
            "            \"title\": \"City\",\n",
            "            \"type\": \"string\"\n",
            "          }\n",
            "        },\n",
            "        \"required\": [\n",
            "          \"city\"\n",
            "        ],\n",
            "        \"title\": \"wheather_args\",\n",
            "        \"type\": \"object\",\n",
            "        \"additionalProperties\": false\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "]\n",
            "Stream: False\n",
            "Tool choice: NOT_GIVEN\n",
            "Response format: NOT_GIVEN\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Calling Litellm model: gemini/gemini-2.0-flash\n",
            "[\n",
            "  {\n",
            "    \"content\": \"You only respond in haikus.\",\n",
            "    \"role\": \"system\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"What`s the wheather of Karachi\"\n",
            "  }\n",
            "]\n",
            "Tools:\n",
            "[\n",
            "  {\n",
            "    \"type\": \"function\",\n",
            "    \"function\": {\n",
            "      \"name\": \"wheather\",\n",
            "      \"description\": \"\",\n",
            "      \"parameters\": {\n",
            "        \"properties\": {\n",
            "          \"city\": {\n",
            "            \"title\": \"City\",\n",
            "            \"type\": \"string\"\n",
            "          }\n",
            "        },\n",
            "        \"required\": [\n",
            "          \"city\"\n",
            "        ],\n",
            "        \"title\": \"wheather_args\",\n",
            "        \"type\": \"object\",\n",
            "        \"additionalProperties\": false\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "]\n",
            "Stream: False\n",
            "Tool choice: NOT_GIVEN\n",
            "Response format: NOT_GIVEN\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM resp:\n",
            "{\n",
            "  \"content\": null,\n",
            "  \"role\": \"assistant\",\n",
            "  \"tool_calls\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"function\": {\n",
            "        \"arguments\": \"{\\\"city\\\": \\\"Karachi\\\"}\",\n",
            "        \"name\": \"wheather\"\n",
            "      },\n",
            "      \"id\": \"call_68170b4a-365c-493d-8985-97a7a5322d2a\",\n",
            "      \"type\": \"function\"\n",
            "    }\n",
            "  ],\n",
            "  \"function_call\": null\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:LLM resp:\n",
            "{\n",
            "  \"content\": null,\n",
            "  \"role\": \"assistant\",\n",
            "  \"tool_calls\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"function\": {\n",
            "        \"arguments\": \"{\\\"city\\\": \\\"Karachi\\\"}\",\n",
            "        \"name\": \"wheather\"\n",
            "      },\n",
            "      \"id\": \"call_68170b4a-365c-493d-8985-97a7a5322d2a\",\n",
            "      \"type\": \"function\"\n",
            "    }\n",
            "  ],\n",
            "  \"function_call\": null\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.FunctionSpanData object at 0x78130c2f9e50>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.FunctionSpanData object at 0x78130c2f9e50>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invoking tool wheather with input {\"city\": \"Karachi\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Invoking tool wheather with input {\"city\": \"Karachi\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool call args: ['Karachi'], kwargs: {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tool call args: ['Karachi'], kwargs: {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Getting wheather for Karachi \n",
            "\n",
            "\n",
            "Tool wheather returned The wheather in Karachi is sunny and warm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tool wheather returned The wheather in Karachi is sunny and warm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running agent Assistant (turn 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Running agent Assistant (turn 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x78130c2f8b90>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x78130c2f8b90>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling Litellm model: gemini/gemini-2.0-flash\n",
            "[\n",
            "  {\n",
            "    \"content\": \"You only respond in haikus.\",\n",
            "    \"role\": \"system\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"What`s the wheather of Karachi\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"assistant\",\n",
            "    \"tool_calls\": [\n",
            "      {\n",
            "        \"id\": \"call_68170b4a-365c-493d-8985-97a7a5322d2a\",\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "          \"name\": \"wheather\",\n",
            "          \"arguments\": \"{\\\"city\\\": \\\"Karachi\\\"}\"\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"tool\",\n",
            "    \"tool_call_id\": \"call_68170b4a-365c-493d-8985-97a7a5322d2a\",\n",
            "    \"content\": \"The wheather in Karachi is sunny and warm\"\n",
            "  }\n",
            "]\n",
            "Tools:\n",
            "[\n",
            "  {\n",
            "    \"type\": \"function\",\n",
            "    \"function\": {\n",
            "      \"name\": \"wheather\",\n",
            "      \"description\": \"\",\n",
            "      \"parameters\": {\n",
            "        \"properties\": {\n",
            "          \"city\": {\n",
            "            \"title\": \"City\",\n",
            "            \"type\": \"string\"\n",
            "          }\n",
            "        },\n",
            "        \"required\": [\n",
            "          \"city\"\n",
            "        ],\n",
            "        \"title\": \"wheather_args\",\n",
            "        \"type\": \"object\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "]\n",
            "Stream: False\n",
            "Tool choice: NOT_GIVEN\n",
            "Response format: NOT_GIVEN\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Calling Litellm model: gemini/gemini-2.0-flash\n",
            "[\n",
            "  {\n",
            "    \"content\": \"You only respond in haikus.\",\n",
            "    \"role\": \"system\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"What`s the wheather of Karachi\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"assistant\",\n",
            "    \"tool_calls\": [\n",
            "      {\n",
            "        \"id\": \"call_68170b4a-365c-493d-8985-97a7a5322d2a\",\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "          \"name\": \"wheather\",\n",
            "          \"arguments\": \"{\\\"city\\\": \\\"Karachi\\\"}\"\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"tool\",\n",
            "    \"tool_call_id\": \"call_68170b4a-365c-493d-8985-97a7a5322d2a\",\n",
            "    \"content\": \"The wheather in Karachi is sunny and warm\"\n",
            "  }\n",
            "]\n",
            "Tools:\n",
            "[\n",
            "  {\n",
            "    \"type\": \"function\",\n",
            "    \"function\": {\n",
            "      \"name\": \"wheather\",\n",
            "      \"description\": \"\",\n",
            "      \"parameters\": {\n",
            "        \"properties\": {\n",
            "          \"city\": {\n",
            "            \"title\": \"City\",\n",
            "            \"type\": \"string\"\n",
            "          }\n",
            "        },\n",
            "        \"required\": [\n",
            "          \"city\"\n",
            "        ],\n",
            "        \"title\": \"wheather_args\",\n",
            "        \"type\": \"object\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "]\n",
            "Stream: False\n",
            "Tool choice: NOT_GIVEN\n",
            "Response format: NOT_GIVEN\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM resp:\n",
            "{\n",
            "  \"content\": \"Karachi is warm now,\\nThe sun shines bright in the sky,\\nA beautiful day.\\n\",\n",
            "  \"role\": \"assistant\",\n",
            "  \"tool_calls\": null,\n",
            "  \"function_call\": null\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:LLM resp:\n",
            "{\n",
            "  \"content\": \"Karachi is warm now,\\nThe sun shines bright in the sky,\\nA beautiful day.\\n\",\n",
            "  \"role\": \"assistant\",\n",
            "  \"tool_calls\": null,\n",
            "  \"function_call\": null\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resetting current trace\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Resetting current trace\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Karachi is warm now,\n",
            "The sun shines bright in the sky,\n",
            "A beautiful day.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YjnkbDlKsMsf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Handoffs:**\n",
        "## is mien hamare pass ik **manager Agent** hota hai jo ke decide karta hai task ki base par ke ye task kis agent ko assign karna hai, jo agent is task ko karne ke capable hoga us ko Manager Agent ye task delegate karde ga."
      ],
      "metadata": {
        "id": "F69wpYJg-pn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Runner, Agent, function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from google.colab import userdata\n",
        "import asyncio\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "@function_tool()\n",
        "def weather(city: str):\n",
        "    print(f\"\\n Getting weather for {city} \\n\\n\")\n",
        "    return f\"The weather in {city} is sunny and warm\"\n",
        "\n",
        "@function_tool()\n",
        "def calculate_square(val: int):\n",
        "    print(\"\\n Calculating square \\n\\n\")\n",
        "    return f\"The square of {val} is {val * val}\"\n",
        "\n",
        "async def tooler(model, api_key):\n",
        "    WeatherAssistant = Agent(\n",
        "        name=\"WeatherAssistant\",\n",
        "        instructions=\"You are a powerful weather assistant. Use the weather tool to provide accurate weather information for the location specified in the user's query.\",\n",
        "        model=LitellmModel(model=model, api_key=api_key),\n",
        "        tools=[weather],\n",
        "        handoff_description=\"Provides current weather conditions, forecasts, and climate information for specific locations.\"\n",
        "    )\n",
        "\n",
        "    Calculator = Agent(\n",
        "        name=\"Calculator\",\n",
        "        instructions=\"You are a powerful math assistant. Use the calculate_square tool to compute the square of the number provided in the user's query.\",\n",
        "        model=LitellmModel(model=model, api_key=api_key),\n",
        "        tools=[calculate_square],\n",
        "        handoff_description=\"Performs mathematical calculations, specifically computing the square of numbers.\"\n",
        "    )\n",
        "\n",
        "    ManagerAgent = Agent(\n",
        "        name=\"Manager\",\n",
        "        instructions=\"You are a manager agent. Analyze the user's query and decide whether to handle it yourself or delegate to a specialized agent. If the query is about weather, use the transfer_to_WeatherAssistant tool. If it involves calculating the square of a number, use the transfer_to_Calculator tool. For other general questions, you can answer directly.\",\n",
        "        model=LitellmModel(model=model, api_key=api_key),\n",
        "        handoffs=[WeatherAssistant, Calculator],\n",
        "    )\n",
        "\n",
        "    result = await Runner.run(\n",
        "        ManagerAgent,\n",
        "        \"What`s the square of 24\"\n",
        "    )\n",
        "\n",
        "    print(result.final_output)\n",
        "\n",
        "asyncio.run(tooler(model=MODEL, api_key=GEMINI_API_KEY))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdEkwbYv_sg8",
        "outputId": "dbda4d88-7bca-410f-d835-109b9d5cab0c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Calculating square \n",
            "\n",
            "\n",
            "The square of 24 is 576.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@function_tool()\n",
        "def weather(city: str):\n",
        "    print(f\"\\n Getting weather for {city} \\n\\n\")\n",
        "    return f\"The weather in {city} is sunny and warm\"\n",
        "\n",
        "@function_tool()\n",
        "def calculate_square(val: int):\n",
        "    print(\"\\n Calculating square \\n\\n\")\n",
        "    return f\"The square of {val} is {val * val}\"\n",
        "\n",
        "\n",
        "WeatherAssistant = Agent(\n",
        "    name=\"WeatherAssistant\",\n",
        "    instructions=\"You are a powerful weather assistant. Use the weather tool to provide accurate weather information for the location specified in the user's query.\",\n",
        "    model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "    tools=[weather],\n",
        "    handoff_description=\"Provides current weather conditions, forecasts, and climate information for specific locations.\"\n",
        ")\n",
        "\n",
        "Calculator = Agent(\n",
        "    name=\"Calculator\",\n",
        "    instructions=\"You are a powerful math assistant. Use the calculate_square tool to compute the square of the number provided in the user's query.\",\n",
        "    model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "    tools=[calculate_square],\n",
        "    handoff_description=\"Performs mathematical calculations, specifically computing the square of numbers.\"\n",
        ")\n",
        "\n",
        "ManagerAgent = Agent(\n",
        "    name=\"Manager\",\n",
        "    instructions=\"You are a manager agent. Analyze the user's query and decide whether to handle it yourself or delegate to a specialized agent. If the query is about weather, use the transfer_to_WeatherAssistant tool. If it involves calculating the square of a number, use the transfer_to_Calculator tool. For other general questions, you can answer directly.\",\n",
        "    model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "    handoffs=[WeatherAssistant, Calculator],\n",
        ")\n",
        "\n",
        "result = await Runner.run(\n",
        "    ManagerAgent,\n",
        "    \"what is the weather of Tokyo\"\n",
        ")\n",
        "\n",
        "print(result.final_output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im-6U94Y92Z0",
        "outputId": "66b3e9df-768d-41c6-ad4a-44274ebc05a0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Getting weather for Tokyo \n",
            "\n",
            "\n",
            "The weather in Tokyo is sunny and warm.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71kcyvJGF8nd",
        "outputId": "52a535d1-1023-4488-d0db-f68571403448"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__abstractmethods__',\n",
              " '__annotations__',\n",
              " '__class__',\n",
              " '__dataclass_fields__',\n",
              " '__dataclass_params__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__match_args__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__slots__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_abc_impl',\n",
              " '_last_agent',\n",
              " 'context_wrapper',\n",
              " 'final_output',\n",
              " 'final_output_as',\n",
              " 'input',\n",
              " 'input_guardrail_results',\n",
              " 'last_agent',\n",
              " 'last_response_id',\n",
              " 'new_items',\n",
              " 'output_guardrail_results',\n",
              " 'raw_responses',\n",
              " 'to_input_list']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.last_agent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imdyu_ohIviY",
        "outputId": "df75b2ad-3eaa-47d5-f1aa-9c0d2e8c2a2a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Agent(name='Calculator', instructions=\"You are a powerful math assistant. Use the calculate_square tool to compute the square of the number provided in the user's query.\", handoff_description='Performs mathematical calculations, specifically computing the square of numbers.', handoffs=[], model=<agents.extensions.models.litellm_model.LitellmModel object at 0x78130ce6b810>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None), tools=[FunctionTool(name='calculate_square', description='', params_json_schema={'properties': {'val': {'title': 'Val', 'type': 'integer'}}, 'required': ['val'], 'title': 'calculate_square_args', 'type': 'object'}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x78131351a020>, strict_json_schema=True)], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.last_agent.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v2xGIN0DJJ7r",
        "outputId": "f708e605-9556-4acc-96d0-91b46c04f2d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Calculator'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Verify which work was done by which agent:**"
      ],
      "metadata": {
        "id": "O-kvCn5mKTdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await Runner.run(\n",
        "    ManagerAgent,\n",
        "    \"what is the square of 26\"\n",
        ")\n",
        "\n",
        "print(result.last_agent.name)\n",
        "print(result.final_output)\n",
        "\n",
        "\n",
        "print(\"__\" * 30)\n",
        "\n",
        "result = await Runner.run(\n",
        "    ManagerAgent,\n",
        "    \"what is the weather of Gujar Khan\"\n",
        ")\n",
        "\n",
        "print(result.last_agent.name)\n",
        "print(result.final_output)\n",
        "\n",
        "\n",
        "print(\"__\" * 30)\n",
        "\n",
        "result = await Runner.run(\n",
        "    ManagerAgent,\n",
        "    \"who is thw founder of Pakistan\"\n",
        ")\n",
        "\n",
        "print(result.last_agent.name)\n",
        "print(result.final_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxN68QSbKIJ8",
        "outputId": "63f10c2e-79d0-4a79-8709-e204406604d1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Calculating square \n",
            "\n",
            "\n",
            "Calculator\n",
            "The square of 26 is 676.\n",
            "\n",
            "____________________________________________________________\n",
            "\n",
            " Getting weather for Gujar Khan \n",
            "\n",
            "\n",
            "WeatherAssistant\n",
            "The weather in Gujar Khan is sunny and warm.\n",
            "\n",
            "____________________________________________________________\n",
            "Manager\n",
            "The founder of Pakistan is Muhammad Ali Jinnah.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Handoff vs Handoffs:**\n",
        "\n",
        "- **Handoff**: Jab aap ek agent se kaam doosre **ek specific agent** ko transfer karte hain, to yeh **handoff** hota hai. Yani, ek single action jisme kaam ek agent se doosre tak jata hai.\n",
        "\n",
        "- **Handoffs**: Yeh tab use hota hai jab aap ek agent ke paas **multiple agents** ke options hote hain jinko woh kaam transfer kar sakta hai. Yani, yeh ek **list** ya collection hai jisme saare possible agents ke naam hote hain jinko handoff kiya ja sakta hai.\n",
        "\n",
        "### **Aasan Alfaaz Mein**\n",
        "- Agar aapko **sirf ek agent** ko kaam dena hai, to aap **handoff** ka use karenge. Maslan, \"Is kaam ko Refund Agent ko do.\"\n",
        "- Agar aapke agent ke paas **multiple agents** ke options hain aur woh decide karega ke kaam kisko dena hai, to yeh **handoffs** ke concept mein aata hai. Maslan, \"Mere paas Refund Agent, Spanish Agent, aur FAQ Agent ke options hain, inme se kisi ko kaam do.\"\n",
        "\n",
        "### **Chhoti Si Baat**\n",
        "Documentation ke hisaab se, **handoffs** ek agent ka property hai jo batata hai ke uske paas kaunse agents available hain kaam transfer karne ke liye. Lekin jab actual kaam transfer hota hai, to woh ek **handoff** action hota hai, chahe ek agent ho ya multiple options mein se ek chuna jaye.\n",
        "\n",
        "Toh aapka point sahi hai: **ek agent ke liye handoff, multiple ke liye handoffs ka concept**. Bas yeh yaad rakhein ke **handoffs** zyada tar options ki list ko represent karta hai, jabke **handoff** action ko.\n",
        "\n",
        "-----------------\n",
        "-----------------\n",
        "-----------------"
      ],
      "metadata": {
        "id": "fJFy-AkyuqjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Handoff_description:**\n",
        "\n",
        "Aasaan alfaaz mein, main aapko batata hoon ke **handoff_description** kya hai aur yeh kya kaam karta hai:\n",
        "\n",
        "### **Handoff_description Kya Hai?**\n",
        "- **Handoff_description** ek property ya field hai jo ek agent ke configuration mein hoti hai jab aap OpenAI Agents SDK use kar rahe hote hain.\n",
        "- Yeh ek **string** hoti hai jo batati hai ke yeh agent kya kaam karta hai ya kis tarah ke tasks ke liye suitable hai.\n",
        "\n",
        "### **Yeh Kya Kaam Karta Hai?**\n",
        "- **Handoff_description** ka maqsad hai ek agent ke role ya purpose ko clearly define karna taake doosre agents (jaise triage agent) samajh sakein ke is agent ko kaunsa kaam dena chahiye.\n",
        "- Jab ek agent (maslan, triage agent) decide karta hai ke kaam kisko handoff karna hai, to woh **handoff_description** ko padhta hai taake yeh pata chal sake ke kaunsa agent us kaam ke liye best hai.\n",
        "- Yeh ek tarah ka label ya summary hota hai jo agent ke kaam ko describe karta hai.\n",
        "\n",
        "### **Example Se Samjhein**\n",
        "Maano aapke paas teen agents hain:\n",
        "1. **Refund Agent**: Iske `handoff_description` mein likha hai: `\"Handles refund requests and processes returns.\"`\n",
        "2. **Spanish Agent**: Iske `handoff_description` mein likha hai: `\"Assists users with queries in Spanish.\"`\n",
        "3. **FAQ Agent**: Iske `handoff_description` mein likha hai: `\"Answers common questions about products.\"`\n",
        "\n",
        "Ab agar ek user ka sawal aata hai ke \"Mujhe refund chahiye,\" to triage agent in sab agents ke **handoff_description** ko dekhega aur samajh jayega ke yeh kaam Refund Agent ke liye hai. Is tarah woh kaam Refund Agent ko handoff kar dega.\n",
        "\n",
        "### **Kaise Use Hota Hai?**\n",
        "Link ke mutabiq, jab aap ek agent banate hain, to aap uska `handoff_description` set karte hain:\n",
        "```python\n",
        "refund_agent = Agent(\n",
        "    name=\"Refund Agent\",\n",
        "    handoff_description=\"Handles refund requests and processes returns.\"\n",
        ")\n",
        "```\n",
        "\n",
        "Yeh description triage agent ke liye ek hint ki tarah kaam karta hai jab woh decide karta hai ke kaam kisko bhejna hai.\n",
        "\n",
        "### **Simple Summary**\n",
        "- **Handoff_description** ek chhota sa explanation hota hai jo batata hai ke ek agent kya karta hai.\n",
        "- Iska kaam hai triage agent ko yeh samjhana ke kaunsa agent kis kaam ke liye perfect hai, taake kaam sahi jagah handoff ho sake.\n",
        "\n",
        "Bas itna hi! Yeh ek tarah ka guide ya tag hai jo handoff process ko asaan banata hai."
      ],
      "metadata": {
        "id": "bLMB5BIlqz7T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SycVdm1suzgs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q:Mujhe yeh samajhna hai ke ManagerAgent ek hi prompt mein do alag tasks—like \"2 ka square kya hai aur Karachi mein weather kya hai?\"—ko pehchan kar unhein alag-alag agents (Calculator aur WeatherAssistant) ko delegate kar sakta hai ya nahi?**\n",
        "\n",
        "## **Answer:** Agar aap chahte hain ke ManagerAgent dono tasks handle kare, to yeh tareeqa hain:\n",
        " - ## 1. **Agents ko as a Tools Dena Manager Agent ko:** ManagerAgent ko khud __calculate_square__ aur __weather__ tools do, taake woh bina delegate kiye dono jawab de sake. Maslan:\n",
        "    - ##  \"2 ka square\" → 4\n",
        "    - ## \"Karachi mein weather\" → (weather data)\n",
        "   ## Phir ManagerAgent khud dono results combine kar ke jawab dega."
      ],
      "metadata": {
        "id": "TEVKaGUaOf7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@function_tool()\n",
        "def weather(city: str):\n",
        "    print(f\"\\n Getting weather for {city} \\n\\n\")\n",
        "    return f\"The weather in {city} is sunny and warm\"\n",
        "\n",
        "@function_tool()\n",
        "def calculate_square(val: int):\n",
        "    print(\"\\n Calculating square \\n\\n\")\n",
        "    return f\"The square of {val} is {val * val}\"\n",
        "\n",
        "\n",
        "WeatherAssistant = Agent(\n",
        "    name=\"WeatherAssistant\",\n",
        "    instructions=\"You are a powerful weather assistant. Use the weather tool to provide accurate weather information for the location specified in the user's query.\",\n",
        "    model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "    tools=[weather],\n",
        "    handoff_description=\"Provides current weather conditions, forecasts, and climate information for specific locations.\"\n",
        ")\n",
        "\n",
        "Calculator = Agent(\n",
        "    name=\"Calculator\",\n",
        "    instructions=\"You are a powerful math assistant. Use the calculate_square tool to compute the square of the number provided in the user's query.\",\n",
        "    model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "    tools=[calculate_square],\n",
        "    handoff_description=\"Performs mathematical calculations, specifically computing the square of numbers.\"\n",
        ")\n",
        "\n",
        "ManagerAgent = Agent(\n",
        "    name=\"Manager\",\n",
        "    instructions=\"You are a manager agent. Analyze the user's query and decide whether to handle it yourself or delegate to a specialized agent. If the query is about weather, use the transfer_to_WeatherAssistant tool. If it involves calculating the square of a number, use the transfer_to_Calculator tool. For other general questions, you can answer directly.\",\n",
        "    model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "    # handoffs=[WeatherAssistant, Calculator],\n",
        "    tools=[\n",
        "        WeatherAssistant.as_tool(tool_name=\"WeatherAssistant\", tool_description=\"Provides current weather conditions, forecasts, and climate information for specific locations.\"),\n",
        "        Calculator.as_tool(tool_name=\"Calculator\", tool_description=\"Performs mathematical calculations, specifically computing the square of numbers.\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "result = await Runner.run(\n",
        "    ManagerAgent,\n",
        "    \"what is the square of 24 and what is te weather in Gujar Khan\"\n",
        ")\n",
        "\n",
        "print(result.final_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB4YdxWcT3PR",
        "outputId": "9bedde82-0ea5-4a18-900e-fab3b38dd121"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Calculating square \n",
            "\n",
            "\n",
            "\n",
            " Getting weather for Gujar Khan \n",
            "\n",
            "\n",
            "The square of 24 is 576. The weather in Gujar Khan is sunny and warm.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aap ne mujhe ek code diya hai aur poocha hai ke kya `ManagerAgent` dono jawab (square of 24 aur weather in Gujar Khan) ek saath de sakta hai. Maine code ko dekha aur run kiya, aur output bhi check kiya. Ab main aap ko batata hoon ke yeh kaise kaam karta hai.\n",
        "\n",
        "Aap ke code mein teen agents hain:\n",
        "- **`WeatherAssistant`**: Yeh ek agent hai jo `weather` tool ke zariye kisi city ka weather batata hai. Abhi ke liye, yeh hamesha \"sunny and warm\" return karta hai.\n",
        "- **`Calculator`**: Yeh ek agent hai jo `calculate_square` tool se kisi number ka square calculate karta hai.\n",
        "- **`ManagerAgent`**: Yeh main agent hai jo user ke query ko analyze karta hai aur decide karta hai ke kaunsa tool use karna hai.\n",
        "\n",
        "`ManagerAgent` ke paas do tools hain:\n",
        "1. `WeatherAssistant` (jo as tool banaya gaya hai)\n",
        "2. `Calculator` (jo bhi as tool banaya gaya hai)\n",
        "\n",
        "Jab aap ka query \"what is the square of 24 and what is te weather in Gujar Khan\" `ManagerAgent` ko diya jata hai, toh yeh query mein do hisson ko pehchanta hai:\n",
        "- \"Square of 24\" → Yeh calculation se related hai.\n",
        "- \"Weather in Gujar Khan\" → Yeh weather se related hai.\n",
        "\n",
        "`ManagerAgent` ke instructions ke mutabiq, woh query ko analyze karta hai aur phir dono tools ko call karta hai:\n",
        "- `Calculator` tool ko 24 ke square ke liye (jo 576 return karta hai).\n",
        "- `WeatherAssistant` tool ko Gujar Khan ke weather ke liye (jo \"sunny and warm\" return karta hai).\n",
        "\n",
        "Phir `ManagerAgent` dono results ko combine karke ek jawab deta hai:  \n",
        "**\"The square of 24 is 576 and the weather in Gujar Khan is sunny and warm.\"**\n",
        "\n",
        "### Yeh Kaise Kaam Karta Hai?\n",
        "- `Agent.as_tool()` method se `WeatherAssistant` aur `Calculator` ko tools mein convert kiya gaya hai.\n",
        "- `ManagerAgent` in tools ko directly call kar sakta hai, bina kisi handoff ke (handoff ka matlab hota hai ke control doosre agent ko de dena, lekin yahan aisa nahi ho raha).\n",
        "- Underlying language model (LLM) query ko samajhta hai aur decide karta hai ke kaunse tools call karne hain. Chunki query mein dono tasks hain, toh dono tools call hote hain.\n",
        "\n",
        "### Aap Ka Output\n",
        "Aap ne jo output diya—\"The square of 24 is 576 and the weather in Gujar Khan is sunny and warm\"—bilkul sahi hai. Yeh dikhata hai ke `ManagerAgent` ne dono kaam ek saath handle kar liye.\n",
        "\n"
      ],
      "metadata": {
        "id": "cLkAwtctWrDm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0gJfUNTTXCuH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Re-run the Loop (most important):__\n",
        "\n",
        "Aap nay mujhse poocha ke \"re-run the loop\" ka kya matlab hai OpenAI Agents SDK ke context mein, aur aap beginner level par samajhna chahte hain. Main aap ko yeh concept bilkul aasaan zubaan mein samjhaata hoon.\n",
        "\n",
        "### \"Re-run the Loop\" Kya Hai?\n",
        "OpenAI Agents SDK mein, jab aap ek agent ko chalate hain (jaise `Runner.run()` method ke zariye), toh yeh ek loop ya chakkar mein kaam karta hai. Yeh loop tab tak chalta rehta hai jab tak agent ko final jawab na mil jaye. Har baar jab loop chalti hai, agent kuch steps follow karta hai:\n",
        "\n",
        "1. **AI Model Ko Call Karna**: Agent pehle Large Language Model (LLM) ko call karta hai, jo AI ka dimagh hota hai. Yeh LLM aap ke sawal ya input ke mutabiq decide karta hai ke aage kya karna hai.\n",
        "2. **Tools Ya Handoffs Ka Istemaal**: LLM decide kar sakta hai ke koi tool use kare (jaise weather check karne ke liye) ya kisi doosre agent ko kaam de (handoff kare).\n",
        "3. **Check Karna Ke Jawab Pura Hua Ya Nahi**: Har action ke baad, agent dekhta hai ke kya yeh final jawab hai. Agar hai, toh loop ruk jati hai. Agar nahi, toh loop dobara chalti hai—ise hi \"re-run the loop\" kehte hain.\n",
        "\n",
        "**\"Re-run the loop\"** ka matlab hai ke agent ko apna process dobara shuru karna padta hai jab tak woh final jawab tak na pohonch jaye. Yeh tab hota hai jab:\n",
        "- **Handoff Hota Hai**: Agar LLM kehta hai ke yeh kaam kisi doosre agent ke liye hai, toh woh handoff karta hai aur loop naye agent ke saath dobara chalti hai.\n",
        "- **Tool Ka Result Aata Hai**: Agar LLM koi tool use karta hai (jaise weather tool), toh tool ka result aane ke baad loop dobara chalti hai taake LLM us result ko samajh sake.\n",
        "\n",
        "Loop tab rukti hai jab LLM final jawab de deta hai bina kisi tool ya handoff ke.\n",
        "\n",
        "### Aasaan Misal\n",
        "Maan lijiye aap agent se poochte hain, \"Tokyo mein weather kya hai?\"\n",
        "\n",
        "1. **Pehla Loop**:\n",
        "   - Agent LLM ko call karta hai.\n",
        "   - LLM kehta hai, \"Mujhe weather tool use karna hai.\"\n",
        "   - Tool kehta hai, \"Tokyo mein sunny aur warm hai.\"\n",
        "   - Abhi final jawab nahi hai, kyunki tool ka result process karna baki hai.\n",
        "\n",
        "2. **Doosra Loop (Re-run)**:\n",
        "   - Agent dobara LLM ko call karta hai, ab tool ke result (\"sunny aur warm\") ke saath.\n",
        "   - LLM dekhta hai ke yeh final jawab hai.\n",
        "   - Loop ruk jati hai aur aap ko jawab milta hai: \"Tokyo mein sunny aur warm hai.\"\n",
        "\n",
        "Is misal mein, loop do baar chali: pehli baar tool ko call karne ke liye, aur doosri baar result ko final jawab banane ke liye. Yeh doosri baar chalna hi \"re-run the loop\" hai.\n",
        "\n",
        "### Beginner Ke Liye Samajh\n",
        "Sochiye agent ek dost hai jo aap ka sawal solve kar raha hai. Woh har baar kuch karta hai—jaise kisi se poochta hai ya koi cheez check karta hai—phir dekhta hai ke kya jawab mil gaya. Agar jawab adhoora hai, toh woh naye information ke saath dobara try karta hai. Yeh dobara try karna hi \"re-run the loop\" hai.\n",
        "\n",
        "Link ke mutabiq, documentation mein likha hai: \"If the LLM does a handoff, we update the current agent and input, and re-run the loop.\" Yani, agar handoff hota hai, toh naye agent aur input ke saath loop dobara chalti hai.\n"
      ],
      "metadata": {
        "id": "OY2ykP2QqSmw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qFyAaBanqYeT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}