{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. install Openai-agents SDK"
      ],
      "metadata": {
        "id": "sZinAhmIwwgN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va-e55o9wauC",
        "outputId": "3397fe37-4ed5-41bb-c5a9-9ea062a9634f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/106.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m102.4/106.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make your Jupyter Notebook capable of running asynchronous functions."
      ],
      "metadata": {
        "id": "6RlstLfYxVCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "-RpsxwuVxCG2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get api keys"
      ],
      "metadata": {
        "id": "_dLcHKY185be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "gemini_api_key = userdata.get(\"GEMINI_API_KEY\")"
      ],
      "metadata": {
        "id": "C6RfY1xHx1Qq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Run Google Gemini with OPENAI-Agent SDK"
      ],
      "metadata": {
        "id": "73BYDNVb9HBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\n",
        "from agents.run import RunConfig"
      ],
      "metadata": {
        "id": "3bkLBq7_7EPQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the API key is present; if not, raise an error\n",
        "if not gemini_api_key:\n",
        "    raise ValueError(\"GEMINI_API_KEY is not set. Please ensure it is defined in your .env file.\")\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client,\n",
        ")\n",
        "\n",
        "config = RunConfig(\n",
        " model=model,\n",
        " model_provider=external_client,\n",
        " tracing_disabled=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "9CLI8evR81O1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **AsyncOpenAI**: ye class `openai-agents SDK` ke ander kisi dosre LLM ko integret karne ke lea use hoti hai.\n",
        "\n",
        "### **OpenAIChatCompletionsModel**: is class mien hum ye btate hai ke hum kon sa model use kar rahe hai aur us ka provider kon hai.\n",
        "\n",
        "### **RunConfige**: is mien hum model aur us ke provider aur OpenAI-Agents SDK ke new features pass kar sakhte hai jaise: `tracing_disabled=True,`configuration jo jab bhi hum agent ko run kare toh us waqt kaam ae gi."
      ],
      "metadata": {
        "id": "j9WlZztgBDKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **Hello world code | method one (syncronously)**"
      ],
      "metadata": {
        "id": "vfBt81iyF1Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent: Agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\", model=model)\n",
        "userinput = input('Enter your question: ')\n",
        "result = Runner.run_sync(agent, input=userinput, run_config=config)\n",
        "\n",
        "print(\"\\nCALLING AGENT\\n\")\n",
        "print(result.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baKRGIhjGAgs",
        "outputId": "3cba2ddd-7e9f-49fd-bbd4-a9b9c0761e48"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: Who is the Founder of Pakistan?\n",
            "\n",
            "CALLING AGENT\n",
            "\n",
            "The founder of Pakistan is generally considered to be **Muhammad Ali Jinnah**.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Hello world code | method two (asyncronously)**:"
      ],
      "metadata": {
        "id": "ElqHpSGeM2XM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import Agent, Runner\n",
        "\n",
        "async def main():\n",
        "  agent: Agent = Agent(name=\"Assistant\", instructions=\"You only respond in haikus.\", model=model)\n",
        "  userinput = input('Enter your question: ')\n",
        "\n",
        "  result = await Runner.run(agent, input=userinput, run_config=config)\n",
        "  # Runner mien .run method by defualt async hota hai.\n",
        "  print(\"\\nCALLING AGENT\\n\")\n",
        "  print(result.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZsgAXydM8zO",
        "outputId": "f1fa48dd-5c00-45d7-db1c-68a7d4890594"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: Tell me about recursion in programming\n",
            "\n",
            "CALLING AGENT\n",
            "\n",
            "A function calls self,\n",
            "Repeating 'til the base case,\n",
            "Problem shrinks down small.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. **Hello world code | method three (streaming):**"
      ],
      "metadata": {
        "id": "nRj4rkZCz8uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Streaming Text code:**\n",
        "is mien hum sirf text ko stream kar rahe hai."
      ],
      "metadata": {
        "id": "GzaUNCgz2HQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "from agents import Agent, Runner\n",
        "\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Joker\",\n",
        "        instructions=\"You are a helpful assistant.\",\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "    result = Runner.run_streamed(agent, input=\"Please tell me 5 jokes.\")\n",
        "    async for event in result.stream_events():\n",
        "      if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
        "          print(event.data.delta, end=\"\", flush=True)\n",
        "\n",
        "\n",
        "\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSsRN0D4pLhp",
        "outputId": "f7f5d277-92cd-416b-cc48-cd2fa81d257a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here are 5 jokes for you:\n",
            "\n",
            "1.  Why don't scientists trust atoms? Because they make up everything!\n",
            "\n",
            "2.  What do you call a lazy kangaroo? Pouch potato!\n",
            "\n",
            "3.  Why did the scarecrow win an award? Because he was outstanding in his field!\n",
            "\n",
            "4.  Parallel lines have so much in common. It’s a shame they’ll never meet.\n",
            "\n",
            "5.  Why don’t eggs tell jokes? They’d crack each other up!\n",
            "\n",
            "Hope you enjoyed them!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yeh code ek asynchronous streaming response handle karne ka tareeqa hai, jahan pe jokes ka jawab stream ki shakal mein aata hai. Aaiye, har line ko asaan alfaaz mein samajhte hain:\n",
        "\n",
        "1. **Result ko stream karna:**\n",
        "   ```python\n",
        "   result = Runner.run_streamed(agent, input=\"Please tell me 5 jokes.\")\n",
        "   ```\n",
        "   - **Kya ho raha hai:** Yeh line `Runner.run_streamed` function ko call kar rahi hai, jismein agent (aapka assistant) ko input message diya gaya hai: \"Please tell me 5 jokes.\" Iska matlab hai ke aap assistant se 5 mazedaar chutkule sunna chahte hain.\n",
        "   - **Output:** Yeh function ek aisa result return karta hai jo stream (data ka chalta hua silsila) ke roop mein hota hai.\n",
        "\n",
        "2. **Stream events ko asynchronously iterate karna:**\n",
        "   ```python\n",
        "   async for event in result.stream_events():\n",
        "   ```\n",
        "   - **Kya ho raha hai:** Yeh loop asynchronous tareeke se stream events (mukhtalif hisse jo response mein aate hain) ko ek ke baad ek process karta hai. Matlab, jaise hi assistant ka jawab thoda thoda karke aata hai, yeh loop usay handle karta hai.\n",
        "\n",
        "3. **Event type aur data check karna:**\n",
        "   ```python\n",
        "   if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
        "   ```\n",
        "   - **Kya ho raha hai:** Har event ko check kiya ja raha hai:\n",
        "     - **`event.type == \"raw_response_event\"`:** Yeh condition ensure karti hai ke event ka type raw response hai, matlab asli text ka hissa.\n",
        "     - **`isinstance(event.data, ResponseTextDeltaEvent)`**: Yeh confirm karta hai ke event ka data, text ka ek chhota hissa (delta) hai.\n",
        "   - **Matlab:** Sirf un events ko process karo jo text ke chhote chhote tukdo (delta) mein response provide kar rahe hon.\n",
        "\n",
        "4. **Response text ko print karna:**\n",
        "   ```python\n",
        "   print(event.data.delta, end=\"\", flush=True)\n",
        "   ```\n",
        "   - **Kya ho raha hai:** Yeh line event ke data se text ka hissa (delta) print kar rahi hai:\n",
        "     - **`end=\"\"`:** Matlab print ke baad newline na lagayein, taake text lagataar (streaming) print hota rahe.\n",
        "     - **`flush=True`:** Yeh ensure karta hai ke print output turant screen par dikh jaye.\n",
        "   - **Matlab:** Jaise hi assistant ka jawab aata hai, woh turant print hota jata hai, bina delay ke.\n",
        "\n",
        "**Overall Concept:**\n",
        "\n",
        "- **Asynchronous Streaming:** Yeh code response ko ek dam ek sath (real-time) stream karta hai, jismein assistant ka jawab chhote chhote hisson mein aata hai aur turant print hota jata hai.\n",
        "- **Event Filtering:** Sirf un events ko process karta hai jo raw text responses hain, is se aapko sahi aur expected output milta hai.\n",
        "\n",
        "Agar koi aur sawaal ho ya koi aur detail chahiye, to aap pooch sakte hain!"
      ],
      "metadata": {
        "id": "Yt1wcwS01GZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Streaming item code:**\n",
        "is mien hum text ke sath jo bi data ho ga us sab ko stream karen ge"
      ],
      "metadata": {
        "id": "zhON7zko2O2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import random\n",
        "\n",
        "from agents import Agent, ItemHelpers, Runner, function_tool\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def how_many_jokes() -> int:\n",
        "    return random.randint(1, 10)\n",
        "\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Joker\",\n",
        "        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n",
        "        tools=[how_many_jokes],\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "    result = Runner.run_streamed(\n",
        "        agent,\n",
        "        input=\"Hello\",\n",
        "\n",
        "    )\n",
        "    print(\"=== Run starting ===\")\n",
        "    async for event in result.stream_events():\n",
        "        # We'll ignore the raw responses event deltas\n",
        "        if event.type == \"raw_response_event\":\n",
        "            continue\n",
        "        elif event.type == \"agent_updated_stream_event\":\n",
        "            print(f\"Agent updated: {event.new_agent.name}\")\n",
        "            continue\n",
        "        elif event.type == \"run_item_stream_event\":\n",
        "            if event.item.type == \"tool_call_item\":\n",
        "                print(\"-- Tool was called\")\n",
        "            elif event.item.type == \"tool_call_output_item\":\n",
        "                print(f\"-- Tool output: {event.item.output}\")\n",
        "            elif event.item.type == \"message_output_item\":\n",
        "                print(f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\")\n",
        "            else:\n",
        "                pass  # Ignore other event types\n",
        "\n",
        "\n",
        "\n",
        "asyncio.run(main())\n",
        "\n",
        "print(\"=== Run complete ===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFOaH3Yj2R7x",
        "outputId": "3d194762-2b38-4731-ab69-294af18a57ed"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Run starting ===\n",
            "Agent updated: Joker\n",
            "-- Tool was called\n",
            "-- Tool output: 7\n",
            "-- Message output:\n",
            " Ok, I will tell you 7 jokes.\n",
            "\n",
            "Why don't scientists trust atoms?\n",
            "Because they make up everything!\n",
            "\n",
            "What do you call a lazy kangaroo?\n",
            "Pouch potato!\n",
            "\n",
            "Why did the bicycle fall over?\n",
            "Because it was two tired!\n",
            "\n",
            "Have you heard about the restaurant on the moon?\n",
            "I heard the food was good but it had no atmosphere.\n",
            "\n",
            "Why did the scarecrow win an award?\n",
            "Because he was outstanding in his field!\n",
            "\n",
            "Why did the teddy bear say no to dessert?\n",
            "Because she was stuffed.\n",
            "\n",
            "What musical instrument is found in the bathroom?\n",
            "A tuba toothpaste.\n",
            "\n",
            "=== Run complete ===\n"
          ]
        }
      ]
    }
  ]
}